{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96edf256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianji/mambaforge/envs/myenv/lib/python3.12/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7414fb",
   "metadata": {},
   "source": [
    "### Read in txt files and modify the column names to distinguish each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e407e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df_and_rename_columns(fisher_test_input_df, suffix, glm_test_input_df=None, \n",
    "                                 add_unique_id=False, FDR=0.05, dI=0.1, coverage=20):\n",
    "    \"\"\"\n",
    "    Processes Fisher test results and optionally adds GLM test results to create a comprehensive output DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fisher_test_input_df : pd.DataFrame\n",
    "        Input DataFrame containing Fisher test results. Must include columns from 'gene' to 'FDR'.\n",
    "        \n",
    "    suffix : str\n",
    "        Suffix to append to Fisher test columns (e.g., \"(suffix)\").\n",
    "        \n",
    "    glm_test_input_df : pd.DataFrame, optional\n",
    "        Input DataFrame containing GLM test results. If provided, adds:\n",
    "        - glm.pvalue: GLM p-values\n",
    "        - glm.FDR: GLM false discovery rates\n",
    "        - glm.filter: GLM significance filter (-1=repressed, 0=insignificant, 1=activated)\n",
    "        \n",
    "    add_unique_id : bool, default False\n",
    "        If True, adds a 'uniqueID' column combining 'name' and 'isoformIDs'.\n",
    "        \n",
    "    FDR : float, default 0.05\n",
    "        False Discovery Rate threshold for significance.\n",
    "        \n",
    "    dI : float, default 0.1\n",
    "        Minimum absolute delta inclusion threshold for activation/repression.\n",
    "        \n",
    "    coverage : int, default 20\n",
    "        Minimum coverage threshold for considering events.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Processed DataFrame with genomic features and Fisher test results. If GLM input is provided,\n",
    "        adds glm.pvalue, glm.FDR, and glm.filter columns.\n",
    "    \"\"\"\n",
    "    # Process Fisher test DataFrame\n",
    "    df_fisher = fisher_test_input_df.copy()\n",
    "    df_fisher = df_fisher.loc[:, 'gene':'FDR'].copy()\n",
    "    \n",
    "    # Add chromLength column after chromEnd\n",
    "    chrom_end_idx = df_fisher.columns.get_loc('chromEnd')\n",
    "    df_fisher.insert(chrom_end_idx + 1, 'chromLength', \n",
    "                     df_fisher['chromEnd'] - df_fisher['chromStart'])\n",
    "    \n",
    "    # Create filter column for Fisher results\n",
    "    df_fisher['filter'] = np.nan\n",
    "    df_fisher.loc[df_fisher['coverage'] > coverage, 'filter'] = 0\n",
    "    df_fisher.loc[\n",
    "        (df_fisher['coverage'] > coverage) & \n",
    "        (df_fisher['dI_g1_vs_g2'] < -dI) & \n",
    "        (df_fisher['FDR'] < FDR),\n",
    "        'filter'\n",
    "    ] = -1\n",
    "    df_fisher.loc[\n",
    "        (df_fisher['coverage'] > coverage) & \n",
    "        (df_fisher['dI_g1_vs_g2'] > dI) & \n",
    "        (df_fisher['FDR'] < FDR),\n",
    "        'filter'\n",
    "    ] = 1\n",
    "    \n",
    "    # Rename Fisher columns with suffix\n",
    "    df_fisher.rename(columns={\n",
    "        'coverage': f'coverage({suffix})',\n",
    "        'dI_g1_vs_g2': f'dI_g1_vs_g2({suffix})',\n",
    "        'pvalue': f'pvalue({suffix})',\n",
    "        'FDR': f'FDR({suffix})',\n",
    "        'filter': f'filter({suffix})'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add unique ID if requested\n",
    "    if add_unique_id:\n",
    "        isoform_idx = df_fisher.columns.get_loc('isoformIDs')\n",
    "        df_fisher.insert(isoform_idx + 1, 'uniqueID', \n",
    "                         df_fisher['name'] + '_' + df_fisher['isoformIDs'])\n",
    "    \n",
    "    # Process GLM test DataFrame if provided\n",
    "    if glm_test_input_df is not None:\n",
    "        df_glm = glm_test_input_df.copy()\n",
    "        # Create GLM filter column\n",
    "        df_glm['glm.filter'] = np.nan\n",
    "        df_glm.loc[df_glm['coverage'] > coverage, 'glm.filter'] = 0\n",
    "        df_glm.loc[\n",
    "            (df_glm['coverage'] > coverage) & \n",
    "            (df_glm['dI_g1_vs_g2'] < -dI) & \n",
    "            (df_glm['FDR'] < FDR),\n",
    "            'glm.filter'\n",
    "        ] = -1\n",
    "        df_glm.loc[\n",
    "            (df_glm['coverage'] > coverage) & \n",
    "            (df_glm['dI_g1_vs_g2'] > dI) & \n",
    "            (df_glm['FDR'] < FDR),\n",
    "            'glm.filter'\n",
    "        ] = 1\n",
    "        \n",
    "        # Create merge key based on whether uniqueID exists\n",
    "        if add_unique_id and 'uniqueID' in df_fisher.columns:\n",
    "            # Create matching uniqueID in GLM DataFrame\n",
    "            df_glm['uniqueID'] = df_glm['name'] + '_' + df_glm['isoformIDs']\n",
    "            merge_key = 'uniqueID'\n",
    "        else:\n",
    "            # Use genomic coordinates as merge key\n",
    "            merge_key = ['gene', 'name', 'chrom', 'strand', 'chromStart', 'chromEnd', 'isoformIDs']\n",
    "        \n",
    "        # Select only the three GLM columns to merge\n",
    "        glm_cols = df_glm[[merge_key] + ['pvalue', 'FDR', 'glm.filter']] \\\n",
    "            if isinstance(merge_key, str) \\\n",
    "            else df_glm[merge_key + ['pvalue', 'FDR', 'glm.filter']]\n",
    "        \n",
    "        # Rename GLM columns\n",
    "        glm_cols = glm_cols.rename(columns={\n",
    "            'pvalue': f'glm.pvalue({suffix})',\n",
    "            'FDR': f'glm.FDR({suffix})',\n",
    "            'glm.filter': f'glm.filter({suffix})'\n",
    "        })\n",
    "        \n",
    "        # Merge with Fisher results\n",
    "        df_fisher = pd.merge(\n",
    "            df_fisher,\n",
    "            glm_cols,\n",
    "            on=merge_key,\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return df_fisher\n",
    "\n",
    "############### Cassette Exon ###############\n",
    "cass_cyc_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Cass.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_g1arr_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Cass.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HEK293_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HEK293.Cass.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_cyc_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Cass.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_g1arr_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Cass.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "    # Add GeneScript Samples\n",
    "cass_HCT116_Ctrl_IAA_0PctKu_IAA_df_fisher = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Cass.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_Ctrl_IAA_0PctKu_IAA_df_glm = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/glm_test_results/HCT116.Cass.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_Ctrl_IAA_10PctKu_IAA_df_fisher = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Cass.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_Ctrl_IAA_10PctKu_IAA_df_glm = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/glm_test_results/HCT116.Cass.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_Ctrl_IAA_50PctKu_flox_df_fisher = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Cass.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_Ctrl_IAA_50PctKu_flox_df_glm = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/glm_test_results/HCT116.Cass.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_50PctKu_flox_0PctKu_flox_df_fisher = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Cass.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_50PctKu_flox_0PctKu_flox_df_glm = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/glm_test_results/HCT116.Cass.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_10PctKu_IAA_0PctKu_IAA_df_fisher = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Cass.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "cass_HCT116_10PctKu_IAA_0PctKu_IAA_df_glm = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Cass.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "\n",
    "cass_HCT116_Ctrl_IAA_0PctKu_IAA_df_fisher = cass_HCT116_Ctrl_IAA_0PctKu_IAA_df_fisher.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.1)'\n",
    "})\n",
    "cass_HCT116_Ctrl_IAA_10PctKu_IAA_df_fisher = cass_HCT116_Ctrl_IAA_10PctKu_IAA_df_fisher.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.2)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_10%Ku.1)'\n",
    "})\n",
    "cass_HCT116_Ctrl_IAA_50PctKu_flox_df_fisher = cass_HCT116_Ctrl_IAA_50PctKu_flox_df_fisher.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.3)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_50%Ku)'\n",
    "})\n",
    "cass_HCT116_50PctKu_flox_0PctKu_flox_df_fisher = cass_HCT116_50PctKu_flox_0PctKu_flox_df_fisher.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_50%KuFlox)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%KuFlox)'\n",
    "})\n",
    "cass_HCT116_10PctKu_IAA_0PctKu_IAA_df_fisher = cass_HCT116_10PctKu_IAA_0PctKu_IAA_df_fisher.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_10%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.2)'\n",
    "})\n",
    "\n",
    "\n",
    "cass_cyc_HCT116_KuKD_df = subset_df_and_rename_columns(cass_cyc_HCT116_KuKD_df, 'Cyc.HCT116.KuKD')\n",
    "cass_g1arr_HCT116_KuKD_df = subset_df_and_rename_columns(cass_g1arr_HCT116_KuKD_df, 'G1Arr.HCT116.KuKD')\n",
    "cass_HEK293_KuKD_df = subset_df_and_rename_columns(cass_HEK293_KuKD_df, 'HEK293.KuKD')\n",
    "cass_cyc_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(cass_cyc_HCT116_DNAPKcsKD_df, 'Cyc.HCT116.DNAPKcsKD')\n",
    "cass_g1arr_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(cass_g1arr_HCT116_DNAPKcsKD_df, 'G1Arr.HCT116.DNAPKcsKD')\n",
    "    # Add GeneScript Samples\n",
    "cass_HCT116_Ctrl_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(fisher_test_input_df = cass_HCT116_Ctrl_IAA_0PctKu_IAA_df_fisher, suffix='HCT116.100%Ku_vs_0%Ku', glm_test_input_df = cass_HCT116_Ctrl_IAA_0PctKu_IAA_df_glm)\n",
    "cass_HCT116_Ctrl_IAA_10PctKu_IAA_df = subset_df_and_rename_columns(fisher_test_input_df = cass_HCT116_Ctrl_IAA_10PctKu_IAA_df_fisher, suffix='HCT116.100%Ku_vs_10%Ku', glm_test_input_df = cass_HCT116_Ctrl_IAA_10PctKu_IAA_df_glm)\n",
    "cass_HCT116_Ctrl_IAA_50PctKu_flox_df = subset_df_and_rename_columns(fisher_test_input_df = cass_HCT116_Ctrl_IAA_50PctKu_flox_df_fisher, suffix='HCT116.100%Ku_vs_50%Ku', glm_test_input_df = cass_HCT116_Ctrl_IAA_50PctKu_flox_df_glm)\n",
    "cass_HCT116_50PctKu_flox_0PctKu_flox_df = subset_df_and_rename_columns(fisher_test_input_df = cass_HCT116_50PctKu_flox_0PctKu_flox_df_fisher, suffix='HCT116.50%Ku_vs_0%Ku', glm_test_input_df = cass_HCT116_50PctKu_flox_0PctKu_flox_df_glm)\n",
    "cass_HCT116_10PctKu_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(fisher_test_input_df = cass_HCT116_10PctKu_IAA_0PctKu_IAA_df_fisher, suffix='HCT116.10%Ku_vs_0%Ku', glm_test_input_df = cass_HCT116_10PctKu_IAA_0PctKu_IAA_df_glm)\n",
    "\n",
    "############### Alternative 3' Splice Site ###############\n",
    "alt3_cyc_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Alt3.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_g1arr_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Alt3.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_HEK293_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HEK293.Alt3.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_cyc_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Alt3.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_g1arr_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Alt3.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "    # Add GeneScript Samples\n",
    "alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt3.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt3.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_HCT116_Ctrl_IAA_50PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt3.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_HCT116_50PctKu_flox_0PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt3.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt3.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df = alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.1)'\n",
    "})\n",
    "alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df = alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.2)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_10%Ku.1)'\n",
    "})\n",
    "alt3_HCT116_Ctrl_IAA_50PctKu_flox_df = alt3_HCT116_Ctrl_IAA_50PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.3)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_50%Ku)'\n",
    "})\n",
    "alt3_HCT116_50PctKu_flox_0PctKu_flox_df = alt3_HCT116_50PctKu_flox_0PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_50%KuFlox)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%KuFlox)'\n",
    "})\n",
    "alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df = alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_10%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.2)'\n",
    "})\n",
    "\n",
    "alt3_cyc_HCT116_KuKD_df = subset_df_and_rename_columns(alt3_cyc_HCT116_KuKD_df, 'Cyc.HCT116.KuKD', add_unique_id = True)\n",
    "alt3_g1arr_HCT116_KuKD_df = subset_df_and_rename_columns(alt3_g1arr_HCT116_KuKD_df, 'G1Arr.HCT116.KuKD', add_unique_id = True)\n",
    "alt3_HEK293_KuKD_df = subset_df_and_rename_columns(alt3_HEK293_KuKD_df, 'HEK293.KuKD', add_unique_id = True)\n",
    "alt3_cyc_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(alt3_cyc_HCT116_DNAPKcsKD_df, 'Cyc.HCT116.DNAPKcsKD', add_unique_id = True)\n",
    "alt3_g1arr_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(alt3_g1arr_HCT116_DNAPKcsKD_df, 'G1Arr.HCT116.DNAPKcsKD', add_unique_id = True)\n",
    "    # Add GeneScript Samples\n",
    "alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df, 'HCT116.100%Ku_vs_0%Ku', add_unique_id = True)\n",
    "alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df = subset_df_and_rename_columns(alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df, 'HCT116.100%Ku_vs_10%Ku', add_unique_id = True)\n",
    "alt3_HCT116_Ctrl_IAA_50PctKu_flox_df = subset_df_and_rename_columns(alt3_HCT116_Ctrl_IAA_50PctKu_flox_df, 'HCT116.100%Ku_vs_50%Ku', add_unique_id = True)\n",
    "alt3_HCT116_50PctKu_flox_0PctKu_flox_df = subset_df_and_rename_columns(alt3_HCT116_50PctKu_flox_0PctKu_flox_df, 'HCT116.50%Ku_vs_0%Ku', add_unique_id = True)\n",
    "alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df, 'HCT116.10%Ku_vs_0%Ku', add_unique_id = True)\n",
    "\n",
    "############### Alternative 5' Splice Site ###############\n",
    "alt5_cyc_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Alt5.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_g1arr_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Alt5.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_HEK293_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HEK293.Alt5.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_cyc_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Alt5.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_g1arr_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Alt5.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "    # Add GeneScript Samples\n",
    "alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt5.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt5.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_HCT116_Ctrl_IAA_50PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt5.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_HCT116_50PctKu_flox_0PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt5.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Alt5.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df = alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.1)'\n",
    "})\n",
    "alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df = alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.2)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_10%Ku.1)'\n",
    "})\n",
    "alt5_HCT116_Ctrl_IAA_50PctKu_flox_df = alt5_HCT116_Ctrl_IAA_50PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.3)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_50%Ku)'\n",
    "})\n",
    "alt5_HCT116_50PctKu_flox_0PctKu_flox_df = alt5_HCT116_50PctKu_flox_0PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_50%KuFlox)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%KuFlox)'\n",
    "})\n",
    "alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df = alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_10%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.2)'\n",
    "})\n",
    "\n",
    "alt5_cyc_HCT116_KuKD_df = subset_df_and_rename_columns(alt5_cyc_HCT116_KuKD_df, 'Cyc.HCT116.KuKD', add_unique_id=True)\n",
    "alt5_g1arr_HCT116_KuKD_df = subset_df_and_rename_columns(alt5_g1arr_HCT116_KuKD_df, 'G1Arr.HCT116.KuKD', add_unique_id=True)\n",
    "alt5_HEK293_KuKD_df = subset_df_and_rename_columns(alt5_HEK293_KuKD_df, 'HEK293.KuKD', add_unique_id=True)\n",
    "alt5_cyc_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(alt5_cyc_HCT116_DNAPKcsKD_df, 'Cyc.HCT116.DNAPKcsKD', add_unique_id=True)\n",
    "alt5_g1arr_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(alt5_g1arr_HCT116_DNAPKcsKD_df, 'G1Arr.HCT116.DNAPKcsKD', add_unique_id=True)\n",
    "    # Add GeneScript Samples\n",
    "alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df, 'HCT116.100%Ku_vs_0%Ku', add_unique_id = True)\n",
    "alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df = subset_df_and_rename_columns(alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df, 'HCT116.100%Ku_vs_10%Ku', add_unique_id = True)\n",
    "alt5_HCT116_Ctrl_IAA_50PctKu_flox_df = subset_df_and_rename_columns(alt5_HCT116_Ctrl_IAA_50PctKu_flox_df, 'HCT116.100%Ku_vs_50%Ku', add_unique_id = True)\n",
    "alt5_HCT116_50PctKu_flox_0PctKu_flox_df = subset_df_and_rename_columns(alt5_HCT116_50PctKu_flox_0PctKu_flox_df, 'HCT116.50%Ku_vs_0%Ku', add_unique_id = True)\n",
    "alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df, 'HCT116.10%Ku_vs_0%Ku', add_unique_id = True)\n",
    "\n",
    "############### Intron Retention ###############\n",
    "iret_cyc_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Iret.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_g1arr_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Iret.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_HEK293_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HEK293.Iret.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_cyc_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Iret.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_g1arr_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Iret.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "    # Add GeneScript Samples\n",
    "iret_HCT116_Ctrl_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Iret.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_HCT116_Ctrl_IAA_10PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Iret.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_HCT116_Ctrl_IAA_50PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Iret.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_HCT116_50PctKu_flox_0PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Iret.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "iret_HCT116_10PctKu_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Iret.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "\n",
    "iret_HCT116_Ctrl_IAA_0PctKu_IAA_df = iret_HCT116_Ctrl_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.1)'\n",
    "})\n",
    "iret_HCT116_Ctrl_IAA_10PctKu_IAA_df = iret_HCT116_Ctrl_IAA_10PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.2)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_10%Ku.1)'\n",
    "})\n",
    "iret_HCT116_Ctrl_IAA_50PctKu_flox_df = iret_HCT116_Ctrl_IAA_50PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.3)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_50%Ku)'\n",
    "})\n",
    "iret_HCT116_50PctKu_flox_0PctKu_flox_df = iret_HCT116_50PctKu_flox_0PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_50%KuFlox)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%KuFlox)'\n",
    "})\n",
    "iret_HCT116_10PctKu_IAA_0PctKu_IAA_df = iret_HCT116_10PctKu_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_10%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.2)'\n",
    "})\n",
    "\n",
    "iret_cyc_HCT116_KuKD_df = subset_df_and_rename_columns(iret_cyc_HCT116_KuKD_df, 'Cyc.HCT116.KuKD')\n",
    "iret_g1arr_HCT116_KuKD_df = subset_df_and_rename_columns(iret_g1arr_HCT116_KuKD_df, 'G1Arr.HCT116.KuKD')\n",
    "iret_HEK293_KuKD_df = subset_df_and_rename_columns(iret_HEK293_KuKD_df, 'HEK293.KuKD')\n",
    "iret_cyc_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(iret_cyc_HCT116_DNAPKcsKD_df, 'Cyc.HCT116.DNAPKcsKD')\n",
    "iret_g1arr_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(iret_g1arr_HCT116_DNAPKcsKD_df, 'G1Arr.HCT116.DNAPKcsKD')\n",
    "    # Add GeneScript Samples\n",
    "iret_HCT116_Ctrl_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(iret_HCT116_Ctrl_IAA_0PctKu_IAA_df, 'HCT116.100%Ku_vs_0%Ku')\n",
    "iret_HCT116_Ctrl_IAA_10PctKu_IAA_df = subset_df_and_rename_columns(iret_HCT116_Ctrl_IAA_10PctKu_IAA_df, 'HCT116.100%Ku_vs_10%Ku')\n",
    "iret_HCT116_Ctrl_IAA_50PctKu_flox_df = subset_df_and_rename_columns(iret_HCT116_Ctrl_IAA_50PctKu_flox_df, 'HCT116.100%Ku_vs_50%Ku')\n",
    "iret_HCT116_50PctKu_flox_0PctKu_flox_df = subset_df_and_rename_columns(iret_HCT116_50PctKu_flox_0PctKu_flox_df, 'HCT116.50%Ku_vs_0%Ku')\n",
    "iret_HCT116_10PctKu_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(iret_HCT116_10PctKu_IAA_0PctKu_IAA_df, 'HCT116.10%Ku_vs_0%Ku')\n",
    "\n",
    "############### Mutually Exclusive Exons ###############\n",
    "mutx_cyc_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Mutx.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_g1arr_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Mutx.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_HEK293_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HEK293.Mutx.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_cyc_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Mutx.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_g1arr_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Mutx.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "    # Add GeneScript Samples\n",
    "mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Mutx.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Mutx.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_HCT116_Ctrl_IAA_50PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Mutx.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_HCT116_50PctKu_flox_0PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Mutx.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Mutx.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "\n",
    "mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df = mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.1)'\n",
    "})\n",
    "mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df = mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.2)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_10%Ku.1)'\n",
    "})\n",
    "mutx_HCT116_Ctrl_IAA_50PctKu_flox_df = mutx_HCT116_Ctrl_IAA_50PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.3)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_50%Ku)'\n",
    "})\n",
    "mutx_HCT116_50PctKu_flox_0PctKu_flox_df = mutx_HCT116_50PctKu_flox_0PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_50%KuFlox)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%KuFlox)'\n",
    "})\n",
    "mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df = mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_10%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.2)'\n",
    "})\n",
    "\n",
    "mutx_cyc_HCT116_KuKD_df = subset_df_and_rename_columns(mutx_cyc_HCT116_KuKD_df, 'Cyc.HCT116.KuKD', add_unique_id=True)\n",
    "mutx_g1arr_HCT116_KuKD_df = subset_df_and_rename_columns(mutx_g1arr_HCT116_KuKD_df, 'G1Arr.HCT116.KuKD', add_unique_id=True)\n",
    "mutx_HEK293_KuKD_df = subset_df_and_rename_columns(mutx_HEK293_KuKD_df, 'HEK293.KuKD', add_unique_id=True)\n",
    "mutx_cyc_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(mutx_cyc_HCT116_DNAPKcsKD_df, 'Cyc.HCT116.DNAPKcsKD', add_unique_id=True)\n",
    "mutx_g1arr_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(mutx_g1arr_HCT116_DNAPKcsKD_df, 'G1Arr.HCT116.DNAPKcsKD', add_unique_id=True)\n",
    "    # Add GeneScript Samples\n",
    "mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df, 'HCT116.100%Ku_vs_0%Ku', add_unique_id = True)\n",
    "mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df = subset_df_and_rename_columns(mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df, 'HCT116.100%Ku_vs_10%Ku', add_unique_id = True)\n",
    "mutx_HCT116_Ctrl_IAA_50PctKu_flox_df = subset_df_and_rename_columns(mutx_HCT116_Ctrl_IAA_50PctKu_flox_df, 'HCT116.100%Ku_vs_50%Ku', add_unique_id = True)\n",
    "mutx_HCT116_50PctKu_flox_0PctKu_flox_df = subset_df_and_rename_columns(mutx_HCT116_50PctKu_flox_0PctKu_flox_df, 'HCT116.50%Ku_vs_0%Ku', add_unique_id = True)\n",
    "mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df, 'HCT116.10%Ku_vs_0%Ku', add_unique_id = True)\n",
    "\n",
    "############### Tandem Cassette Exons ###############\n",
    "taca_cyc_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Taca.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_g1arr_HCT116_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Taca.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_HEK293_KuKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HEK293.Taca.Ctrl.KuKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_cyc_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/Cyc.HCT116.Taca.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_g1arr_HCT116_DNAPKcsKD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/G1Arr.HCT116.Taca.Ctrl.DNAPKcsKD.AlterSpliceResults.txt', delimiter='\\t')\n",
    "    # Add GeneScript Samples\n",
    "taca_HCT116_Ctrl_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Taca.Ctrl_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_HCT116_Ctrl_IAA_10PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Taca.Ctrl_IAA.10PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_HCT116_Ctrl_IAA_50PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Taca.Ctrl_IAA.50PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_HCT116_50PctKu_flox_0PctKu_flox_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Taca.50PctKu_flox.0PctKu_flox.AlterSpliceResults.txt', delimiter='\\t')\n",
    "taca_HCT116_10PctKu_IAA_0PctKu_IAA_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/DSE_Results_Data/fisher_exact_test_results/HCT116.Taca.10PctKu_IAA.0PctKu_IAA.AlterSpliceResults.txt', delimiter='\\t')\n",
    "\n",
    "taca_HCT116_Ctrl_IAA_0PctKu_IAA_df = taca_HCT116_Ctrl_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.1)'\n",
    "})\n",
    "taca_HCT116_Ctrl_IAA_10PctKu_IAA_df = taca_HCT116_Ctrl_IAA_10PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.2)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_10%Ku.1)'\n",
    "})\n",
    "taca_HCT116_Ctrl_IAA_50PctKu_flox_df = taca_HCT116_Ctrl_IAA_50PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_100%Ku.3)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_50%Ku)'\n",
    "})\n",
    "taca_HCT116_50PctKu_flox_0PctKu_flox_df = taca_HCT116_50PctKu_flox_0PctKu_flox_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_50%KuFlox)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%KuFlox)'\n",
    "})\n",
    "taca_HCT116_10PctKu_IAA_0PctKu_IAA_df = taca_HCT116_10PctKu_IAA_0PctKu_IAA_df.rename(columns={\n",
    "    'I_g1(Ctrl)': 'I_g1(Ctrl_10%Ku.1)',\n",
    "    'I_g2(KuKD)': 'I_g2(KuKD_0%Ku.2)'\n",
    "})\n",
    "\n",
    "taca_cyc_HCT116_KuKD_df = subset_df_and_rename_columns(taca_cyc_HCT116_KuKD_df, 'Cyc.HCT116.KuKD')\n",
    "taca_g1arr_HCT116_KuKD_df = subset_df_and_rename_columns(taca_g1arr_HCT116_KuKD_df, 'G1Arr.HCT116.KuKD')\n",
    "taca_HEK293_KuKD_df = subset_df_and_rename_columns(taca_HEK293_KuKD_df, 'HEK293.KuKD')\n",
    "taca_cyc_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(taca_cyc_HCT116_DNAPKcsKD_df, 'Cyc.HCT116.DNAPKcsKD')\n",
    "taca_g1arr_HCT116_DNAPKcsKD_df = subset_df_and_rename_columns(taca_g1arr_HCT116_DNAPKcsKD_df, 'G1Arr.HCT116.DNAPKcsKD')\n",
    "    # Add GeneScript Samples\n",
    "taca_HCT116_Ctrl_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(taca_HCT116_Ctrl_IAA_0PctKu_IAA_df, 'HCT116.100%Ku_vs_0%Ku')\n",
    "taca_HCT116_Ctrl_IAA_10PctKu_IAA_df = subset_df_and_rename_columns(taca_HCT116_Ctrl_IAA_10PctKu_IAA_df, 'HCT116.100%Ku_vs_10%Ku')\n",
    "taca_HCT116_Ctrl_IAA_50PctKu_flox_df = subset_df_and_rename_columns(taca_HCT116_Ctrl_IAA_50PctKu_flox_df, 'HCT116.100%Ku_vs_50%Ku')\n",
    "taca_HCT116_50PctKu_flox_0PctKu_flox_df = subset_df_and_rename_columns(taca_HCT116_50PctKu_flox_0PctKu_flox_df, 'HCT116.50%Ku_vs_0%Ku')\n",
    "taca_HCT116_10PctKu_IAA_0PctKu_IAA_df = subset_df_and_rename_columns(taca_HCT116_10PctKu_IAA_0PctKu_IAA_df, 'HCT116.10%Ku_vs_0%Ku')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d46db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cass_cyc_HCT116_KuKD: 42761 samples\n",
      "cass_g1arr_HCT116_KuKD: 42761 samples\n",
      "cass_HEK293_KuKD: 42761 samples\n",
      "cass_cyc_HCT116_DNAPKcsKD: 42761 samples\n",
      "cass_g1arr_HCT116_DNAPKcsKD: 42761 samples\n",
      "cass_HCT116_100%Ku_vs_0%Ku: 42761 samples\n",
      "cass_HCT116_100%Ku_vs_10%Ku: 42761 samples\n",
      "cass_HCT116_100%Ku_vs_50%Ku: 42761 samples\n",
      "cass_HCT116_50%Ku_vs_0%Ku: 42761 samples\n",
      "cass_HCT116_10%Ku_vs_0%Ku: 42761 samples\n",
      "alt3_cyc_HCT116_KuKD: 20617 samples\n",
      "alt3_g1arr_HCT116_KuKD: 20617 samples\n",
      "alt3_HEK293_KuKD: 20617 samples\n",
      "alt3_cyc_HCT116_DNAPKcsKD: 20617 samples\n",
      "alt3_g1arr_HCT116_DNAPKcsKD: 20617 samples\n",
      "alt3_HCT116_100%Ku_vs_0%Ku: 20617 samples\n",
      "alt3_HCT116_100%Ku_vs_10%Ku: 20617 samples\n",
      "alt3_HCT116_100%Ku_vs_50%Ku: 20617 samples\n",
      "alt3_HCT116_50%Ku_vs_0%Ku: 20617 samples\n",
      "alt3_HCT116_10%Ku_vs_0%Ku: 20617 samples\n",
      "alt5_cyc_HCT116_KuKD: 8066 samples\n",
      "alt5_g1arr_HCT116_KuKD: 8066 samples\n",
      "alt5_HEK293_KuKD: 8066 samples\n",
      "alt5_cyc_HCT116_DNAPKcsKD: 8066 samples\n",
      "alt5_g1arr_HCT116_DNAPKcsKD: 8066 samples\n",
      "alt5_HCT116_100%Ku_vs_0%Ku: 8066 samples\n",
      "alt5_HCT116_100%Ku_vs_10%Ku: 8066 samples\n",
      "alt5_HCT116_100%Ku_vs_50%Ku: 8066 samples\n",
      "alt5_HCT116_50%Ku_vs_0%Ku: 8066 samples\n",
      "alt5_HCT116_10%Ku_vs_0%Ku: 8066 samples\n",
      "iret_cyc_HCT116_KuKD: 5798 samples\n",
      "iret_g1arr_HCT116_KuKD: 5798 samples\n",
      "iret_HEK293_KuKD: 5798 samples\n",
      "iret_cyc_HCT116_DNAPKcsKD: 5798 samples\n",
      "iret_g1arr_HCT116_DNAPKcsKD: 5798 samples\n",
      "iret_HCT116_100%Ku_vs_0%Ku: 5798 samples\n",
      "iret_HCT116_100%Ku_vs_10%Ku: 5798 samples\n",
      "iret_HCT116_100%Ku_vs_50%Ku: 5798 samples\n",
      "iret_HCT116_50%Ku_vs_0%Ku: 5798 samples\n",
      "iret_HCT116_10%Ku_vs_0%Ku: 5798 samples\n",
      "mutx_cyc_HCT116_KuKD: 6683 samples\n",
      "mutx_g1arr_HCT116_KuKD: 6683 samples\n",
      "mutx_HEK293_KuKD: 6683 samples\n",
      "mutx_cyc_HCT116_DNAPKcsKD: 6683 samples\n",
      "mutx_g1arr_HCT116_DNAPKcsKD: 6683 samples\n",
      "mutx_HCT116_100%Ku_vs_0%Ku: 6683 samples\n",
      "mutx_HCT116_100%Ku_vs_10%Ku: 6683 samples\n",
      "mutx_HCT116_100%Ku_vs_50%Ku: 6683 samples\n",
      "mutx_HCT116_50%Ku_vs_0%Ku: 6683 samples\n",
      "mutx_HCT116_10%Ku_vs_0%Ku: 6683 samples\n",
      "taca_cyc_HCT116_KuKD: 18163 samples\n",
      "taca_g1arr_HCT116_KuKD: 18163 samples\n",
      "taca_HEK293_KuKD: 18163 samples\n",
      "taca_cyc_HCT116_DNAPKcsKD: 18163 samples\n",
      "taca_g1arr_HCT116_DNAPKcsKD: 18163 samples\n",
      "taca_HCT116_100%Ku_vs_0%Ku: 18163 samples\n",
      "taca_HCT116_100%Ku_vs_10%Ku: 18163 samples\n",
      "taca_HCT116_100%Ku_vs_50%Ku: 18163 samples\n",
      "taca_HCT116_50%Ku_vs_0%Ku: 18163 samples\n",
      "taca_HCT116_10%Ku_vs_0%Ku: 18163 samples\n"
     ]
    }
   ],
   "source": [
    "# List of all DataFrames and their labels\n",
    "dfs = {\n",
    "    \"cass_cyc_HCT116_KuKD\": cass_cyc_HCT116_KuKD_df,\n",
    "    \"cass_g1arr_HCT116_KuKD\": cass_g1arr_HCT116_KuKD_df,\n",
    "    \"cass_HEK293_KuKD\": cass_HEK293_KuKD_df,\n",
    "    \"cass_cyc_HCT116_DNAPKcsKD\": cass_cyc_HCT116_DNAPKcsKD_df,\n",
    "    \"cass_g1arr_HCT116_DNAPKcsKD\": cass_g1arr_HCT116_DNAPKcsKD_df,\n",
    "    \"cass_HCT116_100%Ku_vs_0%Ku\": cass_HCT116_Ctrl_IAA_0PctKu_IAA_df,\n",
    "    \"cass_HCT116_100%Ku_vs_10%Ku\": cass_HCT116_Ctrl_IAA_10PctKu_IAA_df,\n",
    "    \"cass_HCT116_100%Ku_vs_50%Ku\": cass_HCT116_Ctrl_IAA_50PctKu_flox_df,\n",
    "    \"cass_HCT116_50%Ku_vs_0%Ku\": cass_HCT116_50PctKu_flox_0PctKu_flox_df,\n",
    "    \"cass_HCT116_10%Ku_vs_0%Ku\": cass_HCT116_10PctKu_IAA_0PctKu_IAA_df,\n",
    "\n",
    "\n",
    "    \n",
    "    \"alt3_cyc_HCT116_KuKD\": alt3_cyc_HCT116_KuKD_df,\n",
    "    \"alt3_g1arr_HCT116_KuKD\": alt3_g1arr_HCT116_KuKD_df,\n",
    "    \"alt3_HEK293_KuKD\": alt3_HEK293_KuKD_df,\n",
    "    \"alt3_cyc_HCT116_DNAPKcsKD\": alt3_cyc_HCT116_DNAPKcsKD_df,\n",
    "    \"alt3_g1arr_HCT116_DNAPKcsKD\": alt3_g1arr_HCT116_DNAPKcsKD_df,\n",
    "    \"alt3_HCT116_100%Ku_vs_0%Ku\": alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df,\n",
    "    \"alt3_HCT116_100%Ku_vs_10%Ku\": alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df,\n",
    "    \"alt3_HCT116_100%Ku_vs_50%Ku\": alt3_HCT116_Ctrl_IAA_50PctKu_flox_df,\n",
    "    \"alt3_HCT116_50%Ku_vs_0%Ku\": alt3_HCT116_50PctKu_flox_0PctKu_flox_df,\n",
    "    \"alt3_HCT116_10%Ku_vs_0%Ku\": alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df,\n",
    "\n",
    "    \n",
    "    \"alt5_cyc_HCT116_KuKD\": alt5_cyc_HCT116_KuKD_df,\n",
    "    \"alt5_g1arr_HCT116_KuKD\": alt5_g1arr_HCT116_KuKD_df,\n",
    "    \"alt5_HEK293_KuKD\": alt5_HEK293_KuKD_df,\n",
    "    \"alt5_cyc_HCT116_DNAPKcsKD\": alt5_cyc_HCT116_DNAPKcsKD_df,\n",
    "    \"alt5_g1arr_HCT116_DNAPKcsKD\": alt5_g1arr_HCT116_DNAPKcsKD_df,\n",
    "    \"alt5_HCT116_100%Ku_vs_0%Ku\": alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df,\n",
    "    \"alt5_HCT116_100%Ku_vs_10%Ku\": alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df,\n",
    "    \"alt5_HCT116_100%Ku_vs_50%Ku\": alt5_HCT116_Ctrl_IAA_50PctKu_flox_df,\n",
    "    \"alt5_HCT116_50%Ku_vs_0%Ku\": alt5_HCT116_50PctKu_flox_0PctKu_flox_df,\n",
    "    \"alt5_HCT116_10%Ku_vs_0%Ku\": alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df,\n",
    "    \n",
    "    \"iret_cyc_HCT116_KuKD\": iret_cyc_HCT116_KuKD_df,\n",
    "    \"iret_g1arr_HCT116_KuKD\": iret_g1arr_HCT116_KuKD_df,\n",
    "    \"iret_HEK293_KuKD\": iret_HEK293_KuKD_df,\n",
    "    \"iret_cyc_HCT116_DNAPKcsKD\": iret_cyc_HCT116_DNAPKcsKD_df,\n",
    "    \"iret_g1arr_HCT116_DNAPKcsKD\": iret_g1arr_HCT116_DNAPKcsKD_df,\n",
    "    \"iret_HCT116_100%Ku_vs_0%Ku\": iret_HCT116_Ctrl_IAA_0PctKu_IAA_df,\n",
    "    \"iret_HCT116_100%Ku_vs_10%Ku\": iret_HCT116_Ctrl_IAA_10PctKu_IAA_df,\n",
    "    \"iret_HCT116_100%Ku_vs_50%Ku\": iret_HCT116_Ctrl_IAA_50PctKu_flox_df,\n",
    "    \"iret_HCT116_50%Ku_vs_0%Ku\": iret_HCT116_50PctKu_flox_0PctKu_flox_df,\n",
    "    \"iret_HCT116_10%Ku_vs_0%Ku\": iret_HCT116_10PctKu_IAA_0PctKu_IAA_df,\n",
    "    \n",
    "    \"mutx_cyc_HCT116_KuKD\": mutx_cyc_HCT116_KuKD_df,\n",
    "    \"mutx_g1arr_HCT116_KuKD\": mutx_g1arr_HCT116_KuKD_df,\n",
    "    \"mutx_HEK293_KuKD\": mutx_HEK293_KuKD_df,\n",
    "    \"mutx_cyc_HCT116_DNAPKcsKD\": mutx_cyc_HCT116_DNAPKcsKD_df,\n",
    "    \"mutx_g1arr_HCT116_DNAPKcsKD\": mutx_g1arr_HCT116_DNAPKcsKD_df,\n",
    "    \"mutx_HCT116_100%Ku_vs_0%Ku\": mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df,\n",
    "    \"mutx_HCT116_100%Ku_vs_10%Ku\": mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df,\n",
    "    \"mutx_HCT116_100%Ku_vs_50%Ku\": mutx_HCT116_Ctrl_IAA_50PctKu_flox_df,\n",
    "    \"mutx_HCT116_50%Ku_vs_0%Ku\": mutx_HCT116_50PctKu_flox_0PctKu_flox_df,\n",
    "    \"mutx_HCT116_10%Ku_vs_0%Ku\": mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df,\n",
    "    \n",
    "    \"taca_cyc_HCT116_KuKD\": taca_cyc_HCT116_KuKD_df,\n",
    "    \"taca_g1arr_HCT116_KuKD\": taca_g1arr_HCT116_KuKD_df,\n",
    "    \"taca_HEK293_KuKD\": taca_HEK293_KuKD_df,\n",
    "    \"taca_cyc_HCT116_DNAPKcsKD\": taca_cyc_HCT116_DNAPKcsKD_df,\n",
    "    \"taca_g1arr_HCT116_DNAPKcsKD\": taca_g1arr_HCT116_DNAPKcsKD_df,\n",
    "    \"taca_HCT116_100%Ku_vs_0%Ku\": taca_HCT116_Ctrl_IAA_0PctKu_IAA_df,\n",
    "    \"taca_HCT116_100%Ku_vs_10%Ku\": taca_HCT116_Ctrl_IAA_10PctKu_IAA_df,\n",
    "    \"taca_HCT116_100%Ku_vs_50%Ku\": taca_HCT116_Ctrl_IAA_50PctKu_flox_df,\n",
    "    \"taca_HCT116_50%Ku_vs_0%Ku\": taca_HCT116_50PctKu_flox_0PctKu_flox_df,\n",
    "    \"taca_HCT116_10%Ku_vs_0%Ku\": taca_HCT116_10PctKu_IAA_0PctKu_IAA_df,\n",
    "}\n",
    "\n",
    "\n",
    "# Print the number of samples in each DataFrame\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {len(df)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0f722",
   "metadata": {},
   "source": [
    "### For each of the 6 types of AS, merge 5 different conditions and save the merged file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75a9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_df(df, df2merge, df2merge_suffix, df2merge_Ig1_suffix, df2merge_Ig2_suffix, merge_on_unique_id=False):\n",
    "    \n",
    "    # Base columns to always merge\n",
    "    base_columns = [\n",
    "        f'coverage({df2merge_suffix})', \n",
    "        f'I_g1({df2merge_Ig1_suffix})', \n",
    "        f'I_g2({df2merge_Ig2_suffix})',\n",
    "        f'dI_g1_vs_g2({df2merge_suffix})', \n",
    "        f'pvalue({df2merge_suffix})', \n",
    "        f'FDR({df2merge_suffix})',\n",
    "        f'filter({df2merge_suffix})'\n",
    "    ]\n",
    "    \n",
    "    # GLM columns to merge if they exist\n",
    "    glm_columns = [\n",
    "        f'glm.pvalue({df2merge_suffix})',\n",
    "        f'glm.FDR({df2merge_suffix})',\n",
    "        f'glm.filter({df2merge_suffix})'\n",
    "    ]\n",
    "    \n",
    "    # Check which GLM columns actually exist in df2merge\n",
    "    existing_glm_columns = [col for col in glm_columns if col in df2merge.columns]\n",
    "    \n",
    "    # Combine all columns to merge\n",
    "    all_columns = base_columns + existing_glm_columns\n",
    "    \n",
    "    if not merge_on_unique_id:\n",
    "        # Merge on 'name' column\n",
    "        key_column = 'name'\n",
    "        columns_to_select = [key_column] + all_columns\n",
    "        df2merge_subset = df2merge[columns_to_select]\n",
    "        merged_df = pd.merge(df, df2merge_subset, on=key_column, how='left')\n",
    "    else:\n",
    "        # Merge on 'uniqueID' column\n",
    "        key_column = 'uniqueID'\n",
    "        columns_to_select = [key_column] + all_columns\n",
    "        df2merge_subset = df2merge[columns_to_select]\n",
    "        merged_df = pd.merge(df, df2merge_subset, on=key_column, how='left')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Cassette Exon\n",
    "cass_merged_df = merge_two_df(cass_cyc_HCT116_KuKD_df, cass_g1arr_HCT116_KuKD_df, df2merge_suffix = 'G1Arr.HCT116.KuKD', df2merge_Ig1_suffix = 'G1arr_Ctrl', df2merge_Ig2_suffix = 'G1arr_KuKD')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_HEK293_KuKD_df, df2merge_suffix = 'HEK293.KuKD', df2merge_Ig1_suffix = 'Ctrl', df2merge_Ig2_suffix = 'KuKD')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_cyc_HCT116_DNAPKcsKD_df, df2merge_suffix = 'Cyc.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'Cyc_Ctrl', df2merge_Ig2_suffix = 'Cyc_DNAPKcsKD')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_g1arr_HCT116_DNAPKcsKD_df, df2merge_suffix = 'G1Arr.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'G1Arr_Ctrl', df2merge_Ig2_suffix = 'G1Arr_DNAPKcsKD')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_HCT116_Ctrl_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.1')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_HCT116_Ctrl_IAA_10PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_10%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.2', df2merge_Ig2_suffix = 'KuKD_10%Ku.1')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_HCT116_Ctrl_IAA_50PctKu_flox_df, df2merge_suffix = 'HCT116.100%Ku_vs_50%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.3', df2merge_Ig2_suffix = 'KuKD_50%Ku')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_HCT116_50PctKu_flox_0PctKu_flox_df, df2merge_suffix = 'HCT116.50%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_50%KuFlox', df2merge_Ig2_suffix = 'KuKD_0%KuFlox')\n",
    "cass_merged_df = merge_two_df(cass_merged_df, cass_HCT116_10PctKu_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.10%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_10%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.2')\n",
    "\n",
    "# Alternative 3' Splice Site\n",
    "alt3_merged_df = merge_two_df(alt3_cyc_HCT116_KuKD_df, alt3_g1arr_HCT116_KuKD_df, df2merge_suffix = 'G1Arr.HCT116.KuKD', df2merge_Ig1_suffix = 'G1arr_Ctrl', df2merge_Ig2_suffix = 'G1arr_KuKD', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_HEK293_KuKD_df, df2merge_suffix = 'HEK293.KuKD', df2merge_Ig1_suffix = 'Ctrl', df2merge_Ig2_suffix = 'KuKD', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_cyc_HCT116_DNAPKcsKD_df, df2merge_suffix = 'Cyc.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'Cyc_Ctrl', df2merge_Ig2_suffix = 'Cyc_DNAPKcsKD', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_g1arr_HCT116_DNAPKcsKD_df, df2merge_suffix = 'G1Arr.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'G1Arr_Ctrl', df2merge_Ig2_suffix = 'G1Arr_DNAPKcsKD', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_HCT116_Ctrl_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.1', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_HCT116_Ctrl_IAA_10PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_10%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.2', df2merge_Ig2_suffix = 'KuKD_10%Ku.1', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_HCT116_Ctrl_IAA_50PctKu_flox_df, df2merge_suffix = 'HCT116.100%Ku_vs_50%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.3', df2merge_Ig2_suffix = 'KuKD_50%Ku', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_HCT116_50PctKu_flox_0PctKu_flox_df, df2merge_suffix = 'HCT116.50%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_50%KuFlox', df2merge_Ig2_suffix = 'KuKD_0%KuFlox', merge_on_unique_id = True)\n",
    "alt3_merged_df = merge_two_df(alt3_merged_df, alt3_HCT116_10PctKu_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.10%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_10%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.2', merge_on_unique_id = True)\n",
    "\n",
    "# Alternative 5' Splice Site\n",
    "alt5_merged_df = merge_two_df(alt5_cyc_HCT116_KuKD_df, alt5_g1arr_HCT116_KuKD_df, df2merge_suffix = 'G1Arr.HCT116.KuKD', df2merge_Ig1_suffix = 'G1arr_Ctrl', df2merge_Ig2_suffix = 'G1arr_KuKD', merge_on_unique_id=True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_HEK293_KuKD_df, df2merge_suffix = 'HEK293.KuKD', df2merge_Ig1_suffix = 'Ctrl', df2merge_Ig2_suffix = 'KuKD', merge_on_unique_id = True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_cyc_HCT116_DNAPKcsKD_df, df2merge_suffix = 'Cyc.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'Cyc_Ctrl', df2merge_Ig2_suffix = 'Cyc_DNAPKcsKD', merge_on_unique_id = True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_g1arr_HCT116_DNAPKcsKD_df, df2merge_suffix = 'G1Arr.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'G1Arr_Ctrl', df2merge_Ig2_suffix = 'G1Arr_DNAPKcsKD', merge_on_unique_id=True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_HCT116_Ctrl_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.1', merge_on_unique_id = True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_HCT116_Ctrl_IAA_10PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_10%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.2', df2merge_Ig2_suffix = 'KuKD_10%Ku.1', merge_on_unique_id = True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_HCT116_Ctrl_IAA_50PctKu_flox_df, df2merge_suffix = 'HCT116.100%Ku_vs_50%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.3', df2merge_Ig2_suffix = 'KuKD_50%Ku', merge_on_unique_id = True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_HCT116_50PctKu_flox_0PctKu_flox_df, df2merge_suffix = 'HCT116.50%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_50%KuFlox', df2merge_Ig2_suffix = 'KuKD_0%KuFlox', merge_on_unique_id = True)\n",
    "alt5_merged_df = merge_two_df(alt5_merged_df, alt5_HCT116_10PctKu_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.10%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_10%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.2', merge_on_unique_id = True)\n",
    "\n",
    "# Intron Retention\n",
    "iret_merged_df = merge_two_df(iret_cyc_HCT116_KuKD_df, iret_g1arr_HCT116_KuKD_df, df2merge_suffix = 'G1Arr.HCT116.KuKD', df2merge_Ig1_suffix = 'G1arr_Ctrl', df2merge_Ig2_suffix = 'G1arr_KuKD')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_HEK293_KuKD_df, df2merge_suffix = 'HEK293.KuKD', df2merge_Ig1_suffix = 'Ctrl', df2merge_Ig2_suffix = 'KuKD')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_cyc_HCT116_DNAPKcsKD_df, df2merge_suffix = 'Cyc.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'Cyc_Ctrl', df2merge_Ig2_suffix = 'Cyc_DNAPKcsKD')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_g1arr_HCT116_DNAPKcsKD_df, df2merge_suffix = 'G1Arr.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'G1Arr_Ctrl', df2merge_Ig2_suffix = 'G1Arr_DNAPKcsKD')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_HCT116_Ctrl_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.1')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_HCT116_Ctrl_IAA_10PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_10%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.2', df2merge_Ig2_suffix = 'KuKD_10%Ku.1')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_HCT116_Ctrl_IAA_50PctKu_flox_df, df2merge_suffix = 'HCT116.100%Ku_vs_50%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.3', df2merge_Ig2_suffix = 'KuKD_50%Ku')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_HCT116_50PctKu_flox_0PctKu_flox_df, df2merge_suffix = 'HCT116.50%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_50%KuFlox', df2merge_Ig2_suffix = 'KuKD_0%KuFlox')\n",
    "iret_merged_df = merge_two_df(iret_merged_df, iret_HCT116_10PctKu_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.10%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_10%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.2')\n",
    "\n",
    "# Mutually Exclusive Exons\n",
    "mutx_merged_df = merge_two_df(mutx_cyc_HCT116_KuKD_df, mutx_g1arr_HCT116_KuKD_df, df2merge_suffix = 'G1Arr.HCT116.KuKD', df2merge_Ig1_suffix = 'G1arr_Ctrl', df2merge_Ig2_suffix = 'G1arr_KuKD', merge_on_unique_id=True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_HEK293_KuKD_df, df2merge_suffix = 'HEK293.KuKD', df2merge_Ig1_suffix = 'Ctrl', df2merge_Ig2_suffix = 'KuKD', merge_on_unique_id=True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_cyc_HCT116_DNAPKcsKD_df, df2merge_suffix = 'Cyc.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'Cyc_Ctrl', df2merge_Ig2_suffix = 'Cyc_DNAPKcsKD', merge_on_unique_id=True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_g1arr_HCT116_DNAPKcsKD_df, df2merge_suffix = 'G1Arr.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'G1Arr_Ctrl', df2merge_Ig2_suffix = 'G1Arr_DNAPKcsKD', merge_on_unique_id=True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_HCT116_Ctrl_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.1', merge_on_unique_id = True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_HCT116_Ctrl_IAA_10PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_10%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.2', df2merge_Ig2_suffix = 'KuKD_10%Ku.1', merge_on_unique_id = True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_HCT116_Ctrl_IAA_50PctKu_flox_df, df2merge_suffix = 'HCT116.100%Ku_vs_50%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.3', df2merge_Ig2_suffix = 'KuKD_50%Ku', merge_on_unique_id = True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_HCT116_50PctKu_flox_0PctKu_flox_df, df2merge_suffix = 'HCT116.50%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_50%KuFlox', df2merge_Ig2_suffix = 'KuKD_0%KuFlox', merge_on_unique_id = True)\n",
    "mutx_merged_df = merge_two_df(mutx_merged_df, mutx_HCT116_10PctKu_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.10%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_10%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.2', merge_on_unique_id = True)\n",
    "\n",
    "# Tandem Cassette Exons\n",
    "taca_merged_df = merge_two_df(taca_cyc_HCT116_KuKD_df, taca_g1arr_HCT116_KuKD_df, df2merge_suffix = 'G1Arr.HCT116.KuKD', df2merge_Ig1_suffix = 'G1arr_Ctrl', df2merge_Ig2_suffix = 'G1arr_KuKD')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_HEK293_KuKD_df, df2merge_suffix = 'HEK293.KuKD', df2merge_Ig1_suffix = 'Ctrl', df2merge_Ig2_suffix = 'KuKD')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_cyc_HCT116_DNAPKcsKD_df, df2merge_suffix = 'Cyc.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'Cyc_Ctrl', df2merge_Ig2_suffix = 'Cyc_DNAPKcsKD')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_g1arr_HCT116_DNAPKcsKD_df, df2merge_suffix = 'G1Arr.HCT116.DNAPKcsKD', df2merge_Ig1_suffix = 'G1Arr_Ctrl', df2merge_Ig2_suffix = 'G1Arr_DNAPKcsKD')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_HCT116_Ctrl_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.1')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_HCT116_Ctrl_IAA_10PctKu_IAA_df, df2merge_suffix = 'HCT116.100%Ku_vs_10%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.2', df2merge_Ig2_suffix = 'KuKD_10%Ku.1')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_HCT116_Ctrl_IAA_50PctKu_flox_df, df2merge_suffix = 'HCT116.100%Ku_vs_50%Ku', df2merge_Ig1_suffix = 'Ctrl_100%Ku.3', df2merge_Ig2_suffix = 'KuKD_50%Ku')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_HCT116_50PctKu_flox_0PctKu_flox_df, df2merge_suffix = 'HCT116.50%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_50%KuFlox', df2merge_Ig2_suffix = 'KuKD_0%KuFlox')\n",
    "taca_merged_df = merge_two_df(taca_merged_df, taca_HCT116_10PctKu_IAA_0PctKu_IAA_df, df2merge_suffix = 'HCT116.10%Ku_vs_0%Ku', df2merge_Ig1_suffix = 'Ctrl_10%Ku.1', df2merge_Ig2_suffix = 'KuKD_0%Ku.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc96e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gene', 'chrom', 'chromStart', 'chromEnd', 'chromLength', 'name',\n",
       "       'score', 'strand', 'type', 'isoformIDs', 'coverage(Cyc.HCT116.KuKD)',\n",
       "       'I_g1(Cyc_Ctrl)_x', 'I_g2(Cyc_KuKD)', 'dI_g1_vs_g2(Cyc.HCT116.KuKD)',\n",
       "       'pvalue(Cyc.HCT116.KuKD)', 'FDR(Cyc.HCT116.KuKD)',\n",
       "       'filter(Cyc.HCT116.KuKD)', 'coverage(G1Arr.HCT116.KuKD)',\n",
       "       'I_g1(G1arr_Ctrl)', 'I_g2(G1arr_KuKD)',\n",
       "       'dI_g1_vs_g2(G1Arr.HCT116.KuKD)', 'pvalue(G1Arr.HCT116.KuKD)',\n",
       "       'FDR(G1Arr.HCT116.KuKD)', 'filter(G1Arr.HCT116.KuKD)',\n",
       "       'coverage(HEK293.KuKD)', 'I_g1(Ctrl)', 'I_g2(KuKD)',\n",
       "       'dI_g1_vs_g2(HEK293.KuKD)', 'pvalue(HEK293.KuKD)', 'FDR(HEK293.KuKD)',\n",
       "       'filter(HEK293.KuKD)', 'coverage(Cyc.HCT116.DNAPKcsKD)',\n",
       "       'I_g1(Cyc_Ctrl)_y', 'I_g2(Cyc_DNAPKcsKD)',\n",
       "       'dI_g1_vs_g2(Cyc.HCT116.DNAPKcsKD)', 'pvalue(Cyc.HCT116.DNAPKcsKD)',\n",
       "       'FDR(Cyc.HCT116.DNAPKcsKD)', 'filter(Cyc.HCT116.DNAPKcsKD)',\n",
       "       'coverage(G1Arr.HCT116.DNAPKcsKD)', 'I_g1(G1Arr_Ctrl)',\n",
       "       'I_g2(G1Arr_DNAPKcsKD)', 'dI_g1_vs_g2(G1Arr.HCT116.DNAPKcsKD)',\n",
       "       'pvalue(G1Arr.HCT116.DNAPKcsKD)', 'FDR(G1Arr.HCT116.DNAPKcsKD)',\n",
       "       'filter(G1Arr.HCT116.DNAPKcsKD)', 'coverage(HCT116.100%Ku_vs_0%Ku)',\n",
       "       'I_g1(Ctrl_100%Ku.1)', 'I_g2(KuKD_0%Ku.1)',\n",
       "       'dI_g1_vs_g2(HCT116.100%Ku_vs_0%Ku)', 'pvalue(HCT116.100%Ku_vs_0%Ku)',\n",
       "       'FDR(HCT116.100%Ku_vs_0%Ku)', 'filter(HCT116.100%Ku_vs_0%Ku)',\n",
       "       'glm.pvalue(HCT116.100%Ku_vs_0%Ku)', 'glm.FDR(HCT116.100%Ku_vs_0%Ku)',\n",
       "       'glm.filter(HCT116.100%Ku_vs_0%Ku)', 'coverage(HCT116.100%Ku_vs_10%Ku)',\n",
       "       'I_g1(Ctrl_100%Ku.2)', 'I_g2(KuKD_10%Ku.1)',\n",
       "       'dI_g1_vs_g2(HCT116.100%Ku_vs_10%Ku)', 'pvalue(HCT116.100%Ku_vs_10%Ku)',\n",
       "       'FDR(HCT116.100%Ku_vs_10%Ku)', 'filter(HCT116.100%Ku_vs_10%Ku)',\n",
       "       'glm.pvalue(HCT116.100%Ku_vs_10%Ku)', 'glm.FDR(HCT116.100%Ku_vs_10%Ku)',\n",
       "       'glm.filter(HCT116.100%Ku_vs_10%Ku)',\n",
       "       'coverage(HCT116.100%Ku_vs_50%Ku)', 'I_g1(Ctrl_100%Ku.3)',\n",
       "       'I_g2(KuKD_50%Ku)', 'dI_g1_vs_g2(HCT116.100%Ku_vs_50%Ku)',\n",
       "       'pvalue(HCT116.100%Ku_vs_50%Ku)', 'FDR(HCT116.100%Ku_vs_50%Ku)',\n",
       "       'filter(HCT116.100%Ku_vs_50%Ku)', 'glm.pvalue(HCT116.100%Ku_vs_50%Ku)',\n",
       "       'glm.FDR(HCT116.100%Ku_vs_50%Ku)', 'glm.filter(HCT116.100%Ku_vs_50%Ku)',\n",
       "       'coverage(HCT116.50%Ku_vs_0%Ku)', 'I_g1(Ctrl_50%KuFlox)',\n",
       "       'I_g2(KuKD_0%KuFlox)', 'dI_g1_vs_g2(HCT116.50%Ku_vs_0%Ku)',\n",
       "       'pvalue(HCT116.50%Ku_vs_0%Ku)', 'FDR(HCT116.50%Ku_vs_0%Ku)',\n",
       "       'filter(HCT116.50%Ku_vs_0%Ku)', 'glm.pvalue(HCT116.50%Ku_vs_0%Ku)',\n",
       "       'glm.FDR(HCT116.50%Ku_vs_0%Ku)', 'glm.filter(HCT116.50%Ku_vs_0%Ku)',\n",
       "       'coverage(HCT116.10%Ku_vs_0%Ku)', 'I_g1(Ctrl_10%Ku.1)',\n",
       "       'I_g2(KuKD_0%Ku.2)', 'dI_g1_vs_g2(HCT116.10%Ku_vs_0%Ku)',\n",
       "       'pvalue(HCT116.10%Ku_vs_0%Ku)', 'FDR(HCT116.10%Ku_vs_0%Ku)',\n",
       "       'filter(HCT116.10%Ku_vs_0%Ku)', 'glm.pvalue(HCT116.10%Ku_vs_0%Ku)',\n",
       "       'glm.FDR(HCT116.10%Ku_vs_0%Ku)', 'glm.filter(HCT116.10%Ku_vs_0%Ku)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass_merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2694ba",
   "metadata": {},
   "source": [
    "### Merge Gtex Data, Brain Studer Data, Studerr Data, NVD Data, and Alu tags to cass_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eca8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Tissue psi data downloaded from Gtex to the merged_df\n",
    "gtex_file = '/Users/tianji/Desktop/Alu Project/General Data/GTEx.anno.txt'\n",
    "gtex_df = pd.read_csv(gtex_file, sep='\\t', comment='#')\n",
    "gtex_df.drop('NAME', axis=1, inplace=True)\n",
    "cass_df_with_gtex_psi = pd.merge(cass_merged_df, gtex_df, on='name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2442036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/l_62hzx91kz0hng_60kps5t00000gn/T/ipykernel_62732/1156773686.py:2: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  NMD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/General Data/Exon Information/Exon_size_NMD_TYPE.info.csv')\n"
     ]
    }
   ],
   "source": [
    "# Merge NMD columns to data frames\n",
    "NMD_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/General Data/Exon Information/Exon_size_NMD_TYPE.info.csv')\n",
    "# Select only the needed columns from NMD_df\n",
    "nmd_subset = NMD_df[['name', 'NMD type', 'Ts NMD_ex', 'Ts NMD_in', 'total NMD']]\n",
    "\n",
    "# Merge with cass_df_with_gtex_psi on 'name'\n",
    "cass_df_with_gtex_psi = cass_df_with_gtex_psi.merge(\n",
    "    nmd_subset,\n",
    "    on='name',\n",
    "    how='left'   # use 'inner' if you only want overlapping rows\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517d14ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain span and Studer data has 42761 samples\n",
      "Final Cassette merged df has 42761 samples\n"
     ]
    }
   ],
   "source": [
    "# Merge Brain Span and Studer data to cass_df_with_gtex_psi\n",
    "studer_brainspan_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/analysis/Bryan.neuro.development.info.csv')\n",
    "print(f'Brain span and Studer data has {len(studer_brainspan_df)} samples')\n",
    "# Select columns from 'hPSC' to 'neuron_DIV100'\n",
    "columns_1 = studer_brainspan_df.loc[:, 'hPSC':'neuron_DIV100'].columns.tolist()\n",
    "\n",
    "# Select columns from '2B.10-12pcw' to '11.20+yr'\n",
    "columns_2 = studer_brainspan_df.loc[:, '2B.10-12pcw':'11.20+yr'].columns.tolist()\n",
    "\n",
    "# Combine the two sets of columns, including 'name' for merging\n",
    "columns_to_merge = ['name'] + columns_1 + columns_2\n",
    "\n",
    "# Merge the selected columns into df\n",
    "cass_df_with_gtex_brainspan_studer_psi = cass_df_with_gtex_psi.merge(studer_brainspan_df[columns_to_merge], on='name', how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(f'Final Cassette merged df has {len(cass_df_with_gtex_brainspan_studer_psi)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf97644",
   "metadata": {},
   "source": [
    "### Merge Alu Tags to cass_df_with_gtex_brainspan_studer_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e6d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42761\n"
     ]
    }
   ],
   "source": [
    "# Please refer to \n",
    "merged_ku_overlap_alu_df = pd.read_csv('/Users/tianji/Desktop/Alu Project/Output/Merged_Ku_Overlap_Alu/Merged_Ku_Overlap_Alu.csv')\n",
    "cass_df_with_gtex_brainspan_studer_psi_alu = cass_df_with_gtex_brainspan_studer_psi.merge(merged_ku_overlap_alu_df, on='name', how='left')\n",
    "print(len(cass_df_with_gtex_brainspan_studer_psi_alu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e11b6",
   "metadata": {},
   "source": [
    "### Add a column \"alu_type\" to describe each exon event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46bd925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in multiple categories: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"First define different conditions for asAlu, irAlu, ssAlu and other cassette exons:\n",
    "    --asAlu: 2nd exon contains asAlu\n",
    "    --irAlu: (ui has asAlu and di has ssAlu OR ui has ssAlu and di has asAlu) AND (2nd exon has no asAlu)\n",
    "    --ssAlu: 2nd exon contains ssAlu but not asAlu AND Not (ui has asAlu and di has ssAlu OR ui has ssAlu and di has asAlu)\n",
    "    --other:\n",
    "    (Notation) (1) ui: upstream intron  (2) di: downstream intron\n",
    "    \"\"\"\n",
    "\n",
    "# Simplify the name of cass_df_with_gtex_brainspan_studer_psi_alu\n",
    "cass_df = cass_df_with_gtex_brainspan_studer_psi_alu.copy()\n",
    "\n",
    "# Condition for asAlu\n",
    "asAlu_condition = (cass_df['all_asAlu_exon'] > 0.0)\n",
    "\n",
    "# Two pre-conditions for irAlu\n",
    "irAlu_pre_condition1 = ((cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] > 0.0))\n",
    "irAlu_pre_condition2 = ((cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] > 0.0))\n",
    "# Condition for irAlu\n",
    "irAlu_condition = ((irAlu_pre_condition1 | irAlu_pre_condition2) & \n",
    "                                   (cass_df['all_asAlu_exon'] == 0.0) & (cass_df['all_ssAlu_exon'] == 0.0))\n",
    "\n",
    "# Condition for ssAlu\n",
    "ssAlu_condition = ((cass_df['all_ssAlu_exon'] > 0.0) & \n",
    "                   (cass_df['all_asAlu_exon'] == 0.0))\n",
    "\n",
    "# Condition for other cassette exons\n",
    "other_samples_condition = ~(asAlu_condition | irAlu_condition | ssAlu_condition)\n",
    "\n",
    "# Check for overlapping categories\n",
    "overlap_mask = (\n",
    "    (asAlu_condition & irAlu_condition) |\n",
    "    (asAlu_condition & ssAlu_condition) |\n",
    "    (irAlu_condition & ssAlu_condition)\n",
    ")\n",
    "print(f\"Number of samples in multiple categories: {overlap_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9502ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category counts:\n",
      "alu_type\n",
      "other    24652\n",
      "irAlu    14991\n",
      "asAlu     2678\n",
      "ssAlu      440\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create a new column 'alu_type' after 'isoformIDs' column to annotate the type of each cassette exon event\"\"\"\n",
    "# Create alu_type column using priority: asAlu > irAlu > ssAlu > other\n",
    "conditions = [\n",
    "    asAlu_condition,\n",
    "    irAlu_condition,\n",
    "    ssAlu_condition,\n",
    "    other_samples_condition\n",
    "]\n",
    "\n",
    "choices = ['asAlu', 'irAlu', 'ssAlu', 'other']\n",
    "\n",
    "# Insert new column after 'isoformIDs'\n",
    "cass_df.insert(\n",
    "    loc=cass_df.columns.get_loc('isoformIDs') + 1,\n",
    "    column='alu_type',\n",
    "    value=np.select(conditions[:-1], choices[:-1], default='other')\n",
    ")\n",
    "\n",
    "# Verify distribution\n",
    "print(\"\\nCategory counts:\")\n",
    "print(cass_df['alu_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2c424",
   "metadata": {},
   "source": [
    "### Add a column \"alu_subtype\" to describe each exon event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59b70211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Sum of three asAlu subtypes equal to total asAlu\n",
      "Great! No overlapping samples between asAlu subtypes\n",
      "Great! Sum of three ssAlu subtypes equal to total ssAlu\n",
      "Great! No overlapping samples between ssAlu subtypes\n",
      "Great! Sum of two irAlu subtypes equal to total irAlu\n",
      "Great! No overlapping samples between subtypes\n",
      "Great! Sum of two other samples subtypes equal to total other samples\n",
      "Great! No overlapping samples between subtypes\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "\"\"\"Define different conditions for asAlu subtypes\"\"\"\n",
    "########################################################################\n",
    "\"\"\" --(1) asAlu_not_flanked_by_irAlu: 2nd exon contains asAlu, but ui and di does not contain irAlu\n",
    "    --asAlu_flanked_by_irAlu: 2nd exon contains asAlu and ui and di also contains irAlu pair\n",
    "                                                            |Exon1|-----(ui)-----|Exon2|-----(di)-----|Exon3|\n",
    "        > (2) asAlu_flanked_by_irAlu_without_local_competition:          <=         <=        =>     (irAlu_pre_condition1)\n",
    "                                                                    or   =>         <=        <=     (irAlu_pre_condition2)\n",
    "                                                            --------------------------------------------------\n",
    "        > (3) asAlu_flanked_by_irAlu_with_local_competition:           <=  =>       <=        =>     (subcondition1)\n",
    "                                                                    or   <=         <=      =>  <=   (subcondition2)\n",
    "                                                                    or =>  <=       <=        <=     (subcondition3)\n",
    "                                                                    or   =>         <=      <=  =>   (subcondition4)                                                                                 \n",
    "    (Notation) (1) ui: upstream intron  (2) di: downstream intron\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"Now let's set up conditions to break asAlu type into 3 subtypes\"\"\"\n",
    "# conditions for asAlu not flanked by irAlu\n",
    "asAlu_not_flanked_by_irAlu_condition = ~(irAlu_pre_condition1 | irAlu_pre_condition2) & asAlu_condition\n",
    "\n",
    "# Four types of Local Competing irAlu subconditions\n",
    "subcondition1 = irAlu_pre_condition1 & (cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] == 0.0)\n",
    "subcondition2 = irAlu_pre_condition1 & (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_asAlu_di'] > 0.0)\n",
    "subcondition3 = irAlu_pre_condition2 & (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] == 0.0)\n",
    "subcondition4 = irAlu_pre_condition2 & (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] > 0.0)\n",
    "# Local irAlu Competing conditions\n",
    "local_irAlu_competing_conditions = subcondition1 | subcondition2 | subcondition3 | subcondition4\n",
    "\n",
    "# conditions for asAlu flanked by non-local competing irAlu\n",
    "asAlu_flanked_by_irAlu_without_local_competition = (irAlu_pre_condition1 | irAlu_pre_condition2) & asAlu_condition & ~local_irAlu_competing_conditions\n",
    "\n",
    "# conditions for asAlu flanked by local competing irAlu\n",
    "asAlu_flanked_by_irAlu_with_local_competition = (irAlu_pre_condition1 | irAlu_pre_condition2) & asAlu_condition & local_irAlu_competing_conditions\n",
    "\n",
    "# First verify the conditions\n",
    "total_asAlu = asAlu_condition.sum()\n",
    "sum_subtypes = (asAlu_not_flanked_by_irAlu_condition | \n",
    "               asAlu_flanked_by_irAlu_without_local_competition |\n",
    "               asAlu_flanked_by_irAlu_with_local_competition).sum()\n",
    "\n",
    "condition1_met = (sum_subtypes == total_asAlu)\n",
    "if condition1_met:\n",
    "    print(\"Great! Sum of three asAlu subtypes equal to total asAlu\")\n",
    "else:\n",
    "    print(\"Oh No! There's non-resolved condition. Sum of three asAlu subtypes does not equal to total asAlu\")\n",
    "\n",
    "# Check for overlaps between subtypes\n",
    "overlap1 = (asAlu_not_flanked_by_irAlu_condition & \n",
    "           asAlu_flanked_by_irAlu_without_local_competition).sum()\n",
    "overlap2 = (asAlu_not_flanked_by_irAlu_condition & \n",
    "           asAlu_flanked_by_irAlu_with_local_competition).sum()\n",
    "overlap3 = (asAlu_flanked_by_irAlu_without_local_competition & \n",
    "           asAlu_flanked_by_irAlu_with_local_competition).sum()\n",
    "condition2_met = (overlap1 + overlap2 + overlap3) == 0\n",
    "if condition2_met:\n",
    "    print(f\"Great! No overlapping samples between asAlu subtypes\")\n",
    "else:\n",
    "    print(f\"Oh no! Overlapping samples between asAlu subtypes are found\")\n",
    "\n",
    "\"\"\"Define different conditions for ssAlu subtypes:\n",
    "    --(1) ssAlu_not_flanked_by_irAlu: 2nd exon contains ssAlu, but ui and di does not contain irAlu\n",
    "    --ssAlu_flanked_by_irAlu: 2nd exon contains ssAlu and ui and di also contains irAlu pair\n",
    "                                                            |Exon1|-----(ui)-----|Exon2|-----(di)-----|Exon3|\n",
    "        > (2) ssAlu_flanked_by_irAlu_without_local_competition:          <=         =>        =>     (irAlu_pre_condition1)\n",
    "                                                                    or   =>         =>        <=     (irAlu_pre_condition2)\n",
    "                                                            --------------------------------------------------\n",
    "        > (3) ssAlu_flanked_by_irAlu_with_local_competition:           <=  =>       =>        =>     (subcondition1)\n",
    "                                                                    or   <=         =>      =>  <=   (subcondition2)\n",
    "                                                                    or =>  <=       =>        <=     (subcondition3)\n",
    "                                                                    or   =>         =>      <=  =>   (subcondition4)                                                                                 \n",
    "    (Notation) (1) ui: upstream intron  (2) di: downstream intron\n",
    "    \"\"\"\n",
    "\n",
    "########################################################################\n",
    "\"\"\"Now let's set up conditions to break ssAlu type into 3 subtypes\"\"\"\n",
    "########################################################################\n",
    "# conditions for asAlu not flanked by irAlu\n",
    "ssAlu_not_flanked_by_irAlu_condition = ~(irAlu_pre_condition1 | irAlu_pre_condition2) & ssAlu_condition\n",
    "\n",
    "# Four types of Local Competing irAlu subconditions\n",
    "subcondition1 = irAlu_pre_condition1 & (cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] == 0.0)\n",
    "subcondition2 = irAlu_pre_condition1 & (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_asAlu_di'] > 0.0)\n",
    "subcondition3 = irAlu_pre_condition2 & (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] == 0.0)\n",
    "subcondition4 = irAlu_pre_condition2 & (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] > 0.0)\n",
    "# Local irAlu Competing conditions\n",
    "local_irAlu_competing_conditions = subcondition1 | subcondition2 | subcondition3 | subcondition4\n",
    "\n",
    "# conditions for asAlu flanked by non-local competing irAlu\n",
    "ssAlu_flanked_by_irAlu_without_local_competition = (irAlu_pre_condition1 | irAlu_pre_condition2) & ssAlu_condition & ~local_irAlu_competing_conditions\n",
    "\n",
    "# conditions for asAlu flanked by local competing irAlu\n",
    "ssAlu_flanked_by_irAlu_with_local_competition = (irAlu_pre_condition1 | irAlu_pre_condition2) & ssAlu_condition & local_irAlu_competing_conditions\n",
    "\n",
    "# First verify the conditions\n",
    "total_ssAlu = ssAlu_condition.sum()\n",
    "sum_subtypes = (ssAlu_not_flanked_by_irAlu_condition | \n",
    "               ssAlu_flanked_by_irAlu_without_local_competition |\n",
    "               ssAlu_flanked_by_irAlu_with_local_competition).sum()\n",
    "\n",
    "condition1_met = (sum_subtypes == total_ssAlu)\n",
    "if condition1_met:\n",
    "    print(\"Great! Sum of three ssAlu subtypes equal to total ssAlu\")\n",
    "else:\n",
    "    print(\"Oh No! There's non-resolved condition. Sum of three ssAlu subtypes does not equal to total ssAlu\")\n",
    "\n",
    "# Check for overlaps between subtypes\n",
    "overlap1 = (ssAlu_not_flanked_by_irAlu_condition & \n",
    "           ssAlu_flanked_by_irAlu_without_local_competition).sum()\n",
    "overlap2 = (ssAlu_not_flanked_by_irAlu_condition & \n",
    "           ssAlu_flanked_by_irAlu_with_local_competition).sum()\n",
    "overlap3 = (ssAlu_flanked_by_irAlu_without_local_competition & \n",
    "           ssAlu_flanked_by_irAlu_with_local_competition).sum()\n",
    "condition2_met = (overlap1 + overlap2 + overlap3) == 0\n",
    "if condition2_met:\n",
    "    print(f\"Great! No overlapping samples between ssAlu subtypes\")\n",
    "else:\n",
    "    print(f\"Oh no! Overlapping samples between ssAlu subtypes are found\")\n",
    "\n",
    "########################################################################\n",
    "\"\"\"Now let's set up conditions to break irAlu type into 2 subtypes\"\"\"\n",
    "########################################################################\n",
    "\"\"\"\n",
    "    (1) irAlu without internal competing irAlu: \"irAlu_without_intronic_irAlu_competition\"\n",
    "        |Exon1|-----(ui)-----|Exon2|-----(di)-----|Exon3|\n",
    "                     <=                   =>     (irAlu_no_competition1)\n",
    "      or             =>                   <=     (irAlu_no_competition2)\n",
    "    ----------------------------------------------------------    \n",
    "    (2) irAlu with internal competing irAlu   \n",
    "        |Exon1|-----(ui)-----|Exon2|-----(di)-----|Exon3|                \n",
    "    or             <=  =>                 <=            \n",
    "    or             =>  <=                 <=   \n",
    "    or             <=  =>                 =>            \n",
    "    or             =>  <=                 =>    \n",
    "    or               =>                 <=  =>     \n",
    "    or               =>                 =>  <=     \n",
    "    or               <=                 <=  =>     \n",
    "    or               <=                 =>  <=    \n",
    "\"\"\"\n",
    "# For (1) \n",
    "irAlu_no_competition1 = (cass_df['all_asAlu_exon'] == 0.0) & (cass_df['all_ssAlu_exon'] == 0.0) & (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] > 0.0) & (cass_df['all_asAlu_di'] == 0.0)\n",
    "irAlu_no_competition2 = (cass_df['all_asAlu_exon'] == 0.0) & (cass_df['all_ssAlu_exon'] == 0.0) & (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] == 0.0) & (cass_df['all_asAlu_di'] > 0.0)\n",
    "irAlu_without_intronic_irAlu_competition = irAlu_no_competition1 | irAlu_no_competition2\n",
    "# For (2)\n",
    "ui_irAlu_condition = (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_ssAlu_ui'] > 0.0) \n",
    "di_irAlu_condition = (cass_df['all_asAlu_di'] > 0.0) & (cass_df['all_ssAlu_di'] > 0.0) \n",
    "irAlu_no_competition1 = (cass_df['all_asAlu_exon'] == 0.0) & (cass_df['all_ssAlu_exon'] == 0.0) & ui_irAlu_condition & ((cass_df['all_ssAlu_di'] > 0.0) | (cass_df['all_asAlu_di'] > 0.0))\n",
    "irAlu_no_competition2 = (cass_df['all_asAlu_exon'] == 0.0) & (cass_df['all_ssAlu_exon'] == 0.0) & di_irAlu_condition & ((cass_df['all_ssAlu_ui'] > 0.0) | (cass_df['all_asAlu_ui'] > 0.0))\n",
    "irAlu_with_intronic_irAlu_competition = irAlu_no_competition1 | irAlu_no_competition2 \n",
    "\n",
    "# First verify the conditions\n",
    "total_asAlu = irAlu_condition.sum()\n",
    "sum_subtypes = (irAlu_without_intronic_irAlu_competition | \n",
    "               irAlu_with_intronic_irAlu_competition).sum()\n",
    "\n",
    "condition1_met = (sum_subtypes == total_asAlu)\n",
    "if condition1_met:\n",
    "    print(\"Great! Sum of two irAlu subtypes equal to total irAlu\")\n",
    "else:\n",
    "    print(\"Oh No! There's non-resolved condition. Sum of two irAlu subtypes does not equal to total irAlu\")\n",
    "\n",
    "# Check for overlaps between subtypes\n",
    "overlap = (irAlu_without_intronic_irAlu_competition & \n",
    "           irAlu_with_intronic_irAlu_competition).sum()\n",
    "if overlap == 0:\n",
    "    print(f\"Great! No overlapping samples between subtypes\")\n",
    "else:\n",
    "    print(f\"Oh no! Overlapping samples between subtypes are found\")\n",
    "\n",
    "########################################################################\n",
    "\"\"\"Now let's set up conditions to break other samples type into 2 subtypes\"\"\"\n",
    "########################################################################\n",
    "\"\"\" (1) other samples contain at least one Alu element in each of two flanking introns and in same orientations\n",
    "    |Exon1|-----(ui)-----|Exon2|-----(di)-----|Exon3|\n",
    "                 <=                    <=   \n",
    "      or         =>                    => \n",
    "    (2) other samples contain at least one Alu element in only one flanking intron\n",
    "    |Exon1|-----(ui)-----|Exon2|-----(di)-----|Exon3|\n",
    "                <=                                  (other_with_asAlu_only_ui)\n",
    "      or        =>                                  (other_with_ssAlu_only_ui)\n",
    "      or                              <=            (other_with_asAlu_only_di)       \n",
    "      or                              =>            (other_with_ssAlu_only_di)\n",
    "      or       <= =>                                (other_with_irAlu_only_ui)\n",
    "      or                             <= =>          (other_with_irAlu_only_di)\n",
    "    (3) remaining other samples\n",
    "    \"\"\"\n",
    "other_samples_with_alu_condition = other_samples_condition & (\n",
    "    (cass_df['all_asAlu_ui'] > 0.0) |\n",
    "    (cass_df['all_ssAlu_ui'] > 0.0) |\n",
    "    (cass_df['all_asAlu_di'] > 0.0) |\n",
    "    (cass_df['all_ssAlu_di'] > 0.0)\n",
    ")\n",
    "\n",
    "other_samples_with_alus_in_two_introns_condition = other_samples_condition & (\n",
    "    ((cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] > 0.0)) |\n",
    "    ((cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] > 0.0))\n",
    ")\n",
    "\n",
    "other_samples_with_alus_in_one_intron_condition = other_samples_with_alu_condition & ~other_samples_with_alus_in_two_introns_condition\n",
    "\n",
    "other_samples_without_alu_condition = other_samples_condition & ~other_samples_with_alu_condition\n",
    "\n",
    "combined = (\n",
    "    other_samples_with_alus_in_two_introns_condition |\n",
    "    other_samples_with_alus_in_one_intron_condition |\n",
    "    other_samples_without_alu_condition\n",
    ")\n",
    "other_sample_condition_met = combined.equals(other_samples_condition)\n",
    "\n",
    "if other_sample_condition_met:\n",
    "    print(\"Great! Sum of two other samples subtypes equal to total other samples\")\n",
    "else:\n",
    "    print(\"Oh No! There's non-resolved condition. Sum of two other samples subtypes does not equal to total asAlu\")\n",
    "\n",
    "other_sample_overlap1 = ((other_samples_with_alus_in_two_introns_condition & other_samples_with_alus_in_one_intron_condition).sum() == 0)\n",
    "other_sample_overlap2 = ((other_samples_with_alus_in_two_introns_condition & other_samples_without_alu_condition).sum() == 0)\n",
    "other_sample_overlap3 = ((other_samples_with_alus_in_one_intron_condition & other_samples_without_alu_condition).sum() == 0)\n",
    "other_sample_on_overlap_condition_met = (\n",
    "    other_sample_overlap1 and other_sample_overlap2 and other_sample_overlap3\n",
    ")\n",
    "\n",
    "if other_sample_on_overlap_condition_met:\n",
    "    print(f\"Great! No overlapping samples between subtypes\")\n",
    "else:\n",
    "    print(f\"Oh no! Overlapping samples between subtypes are found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256556d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subtype distribution:\n",
      "alu_subtype\n",
      "irAlu_with_intronic_irAlu_compete                     13770\n",
      "other_with_Alu_in_one_flanking_intron                 13632\n",
      "other_without_Alu                                      9816\n",
      "irAlu_no_intronic_irAlu_compete                        1221\n",
      "other_with_same_strand_Alu_in_two_flanking_introns     1204\n",
      "asAlu_with_flanking_compete_irAlu                      1091\n",
      "asAlu_with_flanking_noncompete_irAlu                    984\n",
      "asAlu_no_flanking_irAlu                                 603\n",
      "ssAlu_no_flanking_irAlu                                 179\n",
      "ssAlu_with_flanking_noncompete_irAlu                    136\n",
      "ssAlu_with_flanking_compete_irAlu                       125\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# If the two condition checks are satified, proceed annotating alu subtypes for asAlu\n",
    "conditions = [\n",
    "    other_samples_with_alus_in_one_intron_condition,\n",
    "    other_samples_with_alus_in_two_introns_condition,\n",
    "    other_samples_without_alu_condition,\n",
    "    asAlu_not_flanked_by_irAlu_condition,\n",
    "    asAlu_flanked_by_irAlu_without_local_competition,\n",
    "    asAlu_flanked_by_irAlu_with_local_competition,\n",
    "    ssAlu_not_flanked_by_irAlu_condition,\n",
    "    ssAlu_flanked_by_irAlu_without_local_competition,\n",
    "    ssAlu_flanked_by_irAlu_with_local_competition,\n",
    "    irAlu_without_intronic_irAlu_competition,\n",
    "    irAlu_with_intronic_irAlu_competition\n",
    "]\n",
    "    \n",
    "choices = [\n",
    "    'other_with_Alu_in_one_flanking_intron',\n",
    "    'other_with_same_strand_Alu_in_two_flanking_introns',\n",
    "    'other_without_Alu',\n",
    "    'asAlu_no_flanking_irAlu',\n",
    "    'asAlu_with_flanking_noncompete_irAlu',\n",
    "    'asAlu_with_flanking_compete_irAlu',\n",
    "    'ssAlu_no_flanking_irAlu',\n",
    "    'ssAlu_with_flanking_noncompete_irAlu',\n",
    "    'ssAlu_with_flanking_compete_irAlu',\n",
    "    'irAlu_no_intronic_irAlu_compete',\n",
    "    'irAlu_with_intronic_irAlu_compete'\n",
    "]\n",
    "    \n",
    "cass_df.insert(\n",
    "    loc=cass_df.columns.get_loc('alu_type') + 1,\n",
    "    column='alu_subtype',\n",
    "    value=np.select(conditions, choices, default='other')\n",
    ")\n",
    "\n",
    "# Verify results\n",
    "print(\"\\nSubtype distribution:\")\n",
    "print(cass_df['alu_subtype'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7137a",
   "metadata": {},
   "source": [
    "### Add a column \"alu_specific_subtype\" to describe each exon event which further break down the \"Other\" subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b82f50c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alu_specific_subtype\n",
       "irAlu_with_intronic_irAlu_compete       13770\n",
       "other_without_Alu                        9816\n",
       "other_with_irAlu_only_di                 3531\n",
       "other_with_irAlu_only_ui                 3287\n",
       "other_with_asAlu_only_di                 1771\n",
       "other_with_ssAlu_only_di                 1768\n",
       "other_with_asAlu_only_ui                 1706\n",
       "other_with_ssAlu_only_ui                 1569\n",
       "irAlu_no_intronic_irAlu_compete          1221\n",
       "asAlu_with_flanking_compete_irAlu        1091\n",
       "asAlu_with_flanking_noncompete_irAlu      984\n",
       "other_with_asAlu_in_both_ui_di            678\n",
       "asAlu_no_flanking_irAlu                   603\n",
       "other_with_ssAlu_in_both_ui_di            526\n",
       "ssAlu_no_flanking_irAlu                   179\n",
       "ssAlu_with_flanking_noncompete_irAlu      136\n",
       "ssAlu_with_flanking_compete_irAlu         125\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base unchanged types\n",
    "unchanged_subtypes = [\n",
    "    'ssAlu',\n",
    "    'other_without_Alu',\n",
    "    'asAlu_no_flanking_irAlu',\n",
    "    'asAlu_with_flanking_noncompete_irAlu',\n",
    "    'asAlu_with_flanking_compete_irAlu',\n",
    "    'irAlu_no_intronic_irAlu_compete',\n",
    "    'irAlu_with_intronic_irAlu_compete'\n",
    "]\n",
    "\n",
    "# Start with the existing 'alu_subtype' column\n",
    "cass_df.insert(\n",
    "    loc=cass_df.columns.get_loc('alu_subtype') + 1,\n",
    "    column='alu_specific_subtype',\n",
    "    value=cass_df['alu_subtype']\n",
    ")\n",
    "\n",
    "# === Split subtype: 'other_with_Alu_in_one_flanking_intron' ===\n",
    "\n",
    "# Define sub-conditions\n",
    "asAlu_only_ui = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_Alu_in_one_flanking_intron') &\n",
    "    (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] == 0.0) &\n",
    "    (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] == 0.0)\n",
    ")\n",
    "\n",
    "asAlu_only_di = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_Alu_in_one_flanking_intron') &\n",
    "    (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_asAlu_di'] > 0.0) &\n",
    "    (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] == 0.0)\n",
    ")\n",
    "\n",
    "ssAlu_only_ui = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_Alu_in_one_flanking_intron') &\n",
    "    (cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] == 0.0) &\n",
    "    (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_asAlu_di'] == 0.0)\n",
    ")\n",
    "\n",
    "ssAlu_only_di = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_Alu_in_one_flanking_intron') &\n",
    "    (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] > 0.0) &\n",
    "    (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_asAlu_di'] == 0.0)\n",
    ")\n",
    "\n",
    "irAlu_only_ui = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_Alu_in_one_flanking_intron') &\n",
    "    (cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] == 0.0) &\n",
    "    (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] == 0.0)\n",
    ")\n",
    "\n",
    "irAlu_only_di = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_Alu_in_one_flanking_intron') &\n",
    "    (cass_df['all_ssAlu_ui'] == 0.0) & (cass_df['all_ssAlu_di'] > 0.0) &\n",
    "    (cass_df['all_asAlu_ui'] == 0.0) & (cass_df['all_asAlu_di'] > 0.0)\n",
    ")\n",
    "\n",
    "# Apply annotations\n",
    "cass_df.loc[asAlu_only_ui, 'alu_specific_subtype'] = 'other_with_asAlu_only_ui'\n",
    "cass_df.loc[asAlu_only_di, 'alu_specific_subtype'] = 'other_with_asAlu_only_di'\n",
    "cass_df.loc[ssAlu_only_ui, 'alu_specific_subtype'] = 'other_with_ssAlu_only_ui'\n",
    "cass_df.loc[ssAlu_only_di, 'alu_specific_subtype'] = 'other_with_ssAlu_only_di'\n",
    "cass_df.loc[irAlu_only_ui, 'alu_specific_subtype'] = 'other_with_irAlu_only_ui'\n",
    "cass_df.loc[irAlu_only_di, 'alu_specific_subtype'] = 'other_with_irAlu_only_di'\n",
    "\n",
    "# === Split subtype: 'other_with_same_strand_Alu_in_two_flanking_introns' ===\n",
    "\n",
    "asAlu_both = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_same_strand_Alu_in_two_flanking_introns') &\n",
    "    (cass_df['all_asAlu_ui'] > 0.0) & (cass_df['all_asAlu_di'] > 0.0)\n",
    ")\n",
    "\n",
    "ssAlu_both = (\n",
    "    (cass_df['alu_subtype'] == 'other_with_same_strand_Alu_in_two_flanking_introns') &\n",
    "    (cass_df['all_ssAlu_ui'] > 0.0) & (cass_df['all_ssAlu_di'] > 0.0)\n",
    ")\n",
    "\n",
    "cass_df.loc[asAlu_both, 'alu_specific_subtype'] = 'other_with_asAlu_in_both_ui_di'\n",
    "cass_df.loc[ssAlu_both, 'alu_specific_subtype'] = 'other_with_ssAlu_in_both_ui_di'\n",
    "\n",
    "cass_df['alu_specific_subtype'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb0dec",
   "metadata": {},
   "source": [
    "### Add a column \"ku_tag_summary\" to summarize Ku tag numbers in ui, exon, di respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a00f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Insert Ku_tag_summary column to summarize ku tag numebers\"\"\"\n",
    "cass_df['ku_tag_summary'] = (\n",
    "    'asUI=' + cass_df['Ku_asAlu_ui'].astype(str) + ', ' +\n",
    "    'ssUI=' + cass_df['Ku_ssAlu_ui'].astype(str) + ', ' +\n",
    "    'asExon=' + cass_df['Ku_asAlu_exon'].astype(str) + ', ' +\n",
    "    'ssExon=' + cass_df['Ku_ssAlu_exon'].astype(str) + ', ' +\n",
    "    'asDI=' + cass_df['Ku_asAlu_di'].astype(str) + ', ' +\n",
    "    'ssDI=' + cass_df['Ku_ssAlu_di'].astype(str)\n",
    ")\n",
    "\n",
    "cass_df.insert(\n",
    "    loc=cass_df.columns.get_loc('alu_specific_subtype') + 1,\n",
    "    column='ku_tag_summary',\n",
    "    value=cass_df.pop('ku_tag_summary')  # remove & re-insert\n",
    ")\n",
    "\n",
    "\"\"\" Insert Ku_bound column to summarize whether a cassette exon has Ku tags bind to it\n",
    "    --for asAlus, Ku_bound = 1 if ku_asAlu_exon > 0.0\n",
    "    --for ssAlus, Ku_bound = 1 if ku_ssAlu_exon > 0.0\n",
    "    --for irAlus and other, Ku_bound = 1 if any of [ku_asAlu_ui,ku_asAlu_di,ku_ssAlu_ui,ku_ssAlu_di] > 0.0\n",
    "    \"\"\"\n",
    "cass_df['ku_bound'] = 0\n",
    "\n",
    "# Apply rules based on alu_type\n",
    "cass_df.loc[\n",
    "    (cass_df['alu_type'] == 'asAlu') & (cass_df['Ku_asAlu_exon'] > 0.0),\n",
    "    'ku_bound'\n",
    "] = 1\n",
    "\n",
    "cass_df.loc[\n",
    "    (cass_df['alu_type'] == 'ssAlu') & (cass_df['Ku_ssAlu_exon'] > 0.0),\n",
    "    'ku_bound'\n",
    "] = 1\n",
    "\n",
    "cass_df.loc[\n",
    "    (cass_df['alu_type'].isin(['irAlu', 'other'])) &\n",
    "    (\n",
    "        (cass_df['Ku_asAlu_ui'] > 0.0) |\n",
    "        (cass_df['Ku_asAlu_di'] > 0.0) |\n",
    "        (cass_df['Ku_ssAlu_ui'] > 0.0) |\n",
    "        (cass_df['Ku_ssAlu_di'] > 0.0)\n",
    "    ),\n",
    "    'ku_bound'\n",
    "] = 1\n",
    "\n",
    "# Step 4: Insert ku_bound after ku_tag_summary\n",
    "cass_df.insert(\n",
    "    loc=cass_df.columns.get_loc('ku_tag_summary') + 1,\n",
    "    column='ku_bound',\n",
    "    value=cass_df.pop('ku_bound')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c0b804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ku_bound\n",
       "0    21800\n",
       "1    20961\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass_df['ku_bound'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82266c",
   "metadata": {},
   "source": [
    "### Add a column '100vs0_10vs0_Overlap' to indicate whether a DSE belongs to 100vs0 or 10vs0 or both conditions. Also add a column 'sensitivity' to categorize DSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0dc08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filter condition\n",
    "filter_condition = (\n",
    "    cass_df['filter(HCT116.100%Ku_vs_0%Ku)'].isin([-1, 1]) |\n",
    "    cass_df['filter(HCT116.10%Ku_vs_0%Ku)'].isin([-1, 1])\n",
    ")\n",
    "\n",
    "# Define opposite direction condition\n",
    "opposite_direction_condition = (\n",
    "    (cass_df['dI_g1_vs_g2(HCT116.10%Ku_vs_0%Ku)'] > 0) & \n",
    "    (cass_df['dI_g1_vs_g2(HCT116.100%Ku_vs_0%Ku)'] < 0)\n",
    ") | (\n",
    "    (cass_df['dI_g1_vs_g2(HCT116.10%Ku_vs_0%Ku)'] < 0) & \n",
    "    (cass_df['dI_g1_vs_g2(HCT116.100%Ku_vs_0%Ku)'] > 0)\n",
    ")\n",
    "\n",
    "# Determine overlap types\n",
    "is_100 = cass_df['filter(HCT116.100%Ku_vs_0%Ku)'].isin([-1, 1])\n",
    "is_10 = cass_df['filter(HCT116.10%Ku_vs_0%Ku)'].isin([-1, 1])\n",
    "\n",
    "overlap_annotation = pd.Series('', index=cass_df.index)\n",
    "overlap_annotation[is_100 & ~is_10] = '100% vs 0% only'\n",
    "overlap_annotation[is_10 & ~is_100] = '10% vs 0% only'\n",
    "overlap_annotation[is_100 & is_10] = 'common'\n",
    "\n",
    "# Insert 100vs0_10vs0_Overlap column\n",
    "cass_df.insert(\n",
    "    cass_df.columns.get_loc('alu_subtype') + 1,\n",
    "    '100vs0_10vs0_Overlap',\n",
    "    overlap_annotation\n",
    ")\n",
    "\n",
    "# Initialize sensitivity annotation\n",
    "sensitivity = pd.Series('', index=cass_df.index)\n",
    "\n",
    "# Apply sensitivity annotation rules\n",
    "common = cass_df['100vs0_10vs0_Overlap'] == 'common'\n",
    "opposite = opposite_direction_condition\n",
    "\n",
    "sensitivity[filter_condition & common] = 'high'\n",
    "sensitivity[filter_condition & common & opposite] = 'outlier'\n",
    "sensitivity[filter_condition & ~common & opposite] = 'low'\n",
    "sensitivity[filter_condition & ~common & ~opposite] = 'middle'\n",
    "\n",
    "# Insert sensitivity column\n",
    "cass_df.insert(\n",
    "    cass_df.columns.get_loc('100vs0_10vs0_Overlap') + 1,\n",
    "    'sensitivity',\n",
    "    sensitivity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a44e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensitivity\n",
       "          16950\n",
       "middle      556\n",
       "high        481\n",
       "low         122\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass_df[cass_df['alu_type'].isin(['asAlu', 'irAlu', 'ssAlu'])]['sensitivity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91608092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensitivity\n",
       "           40124\n",
       "middle      1388\n",
       "high         967\n",
       "low          281\n",
       "outlier        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass_df['sensitivity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d533e954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100vs0_10vs0_Overlap\n",
       "                   16950\n",
       "common               481\n",
       "100% vs 0% only      413\n",
       "10% vs 0% only       265\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass_df[cass_df['alu_type'].isin(['asAlu', 'irAlu', 'ssAlu'])]['100vs0_10vs0_Overlap'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7d081",
   "metadata": {},
   "source": [
    "### Add a column \"exon_sizes\" to describe each length of each exon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d22288e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cass_df_copy = cass_df.copy()\n",
    "\n",
    "bed_columns = [\n",
    "    'chrom', 'start', 'end', 'name', 'score', 'strand',\n",
    "    'thick_start', 'thick_end', 'rgb', 'block_count', 'exon_sizes', 'block_starts'\n",
    "]\n",
    "# Read the BED file\n",
    "cass_anno_bed_file_path = \"/Users/tianji/Desktop/Alu Project/CLIP Data/Alu-CassExon Reference Data/Hs.seq.all.cass.chrom.can.bed\"\n",
    "cass_anno = pd.read_csv(cass_anno_bed_file_path, sep='\\t', header=None, names=bed_columns)\n",
    "\n",
    "# Merge cass_anno's 'exon_sizes' into cass_df after 'chromLength'\n",
    "cass_df = pd.merge(\n",
    "    cass_df_copy,\n",
    "    cass_anno[['name', 'exon_sizes']],  # Select only 'name' and 'exon_sizes' from cass_anno\n",
    "    on='name',                           # Merge on the 'name' column\n",
    "    how='left'                           # Keep all rows from cass_df even if no match\n",
    ")\n",
    "\n",
    "# Reorder columns to place 'exon_sizes' after 'chromLength'\n",
    "cols = cass_df.columns.tolist()\n",
    "chromLength_idx = cols.index('chromLength')\n",
    "cols.insert(chromLength_idx + 1, cols.pop(cols.index('exon_sizes')))  # Move 'exon_sizes' after 'chromLength'\n",
    "cass_df = cass_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febb26a",
   "metadata": {},
   "source": [
    "### Add columns \"all_asAlu_type\", \"all_asAlu_id\" and \"all_asAlu_family\" to indicate what asAlu elements overlap with the middle exon in each cassette exon event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8db5cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "asAlu2exon = pd.read_csv(\"/Users/tianji/Desktop/Alu Project/Alu_Family_Analysis/asAlu.Overlap.CassMiddleExon.csv\")\n",
    "cass_df_copy = cass_df.copy()\n",
    "\n",
    "# Step 1: Rename Alu-related columns in asAlu2exon to avoid conflicts with column names in cass_df_copy before merge\n",
    "asAlu2exon = asAlu2exon.rename(columns={\n",
    "    'Alu_type': 'all_asAlu_type',\n",
    "    'Alu_id': 'all_asAlu_id',\n",
    "    'Alu_family': 'all_asAlu_family',\n",
    "    'ku_regulated': 'all_asAlu_Ku_regulated'\n",
    "})\n",
    "\n",
    "# Step 2: Merge with cass_df_copy on 'name'\n",
    "cass_df_copy = cass_df_copy.merge(\n",
    "    asAlu2exon[['name', 'all_asAlu_type', 'all_asAlu_id', 'all_asAlu_family', 'all_asAlu_Ku_regulated']],\n",
    "    on='name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 3: Insert 'all_asAlu_type', 'all_asAlu_id', 'all_asAlu_family', and 'all_asAlu_Ku_regulated' after the 'all_asAlu_exon' column in cass_df_copy\n",
    "new_cols = ['all_asAlu_type', 'all_asAlu_id', 'all_asAlu_family', 'all_asAlu_Ku_regulated']\n",
    "\n",
    "# Find the insertion index just after 'all_asAlu_exon'\n",
    "cols = list(cass_df_copy.columns)\n",
    "insert_at = cols.index('all_asAlu_exon') + 1\n",
    "\n",
    "# Pop & insert each new column in the right order\n",
    "for col in new_cols:\n",
    "    # .pop(col) removes the column and returns its Series\n",
    "    cass_df_copy.insert(insert_at, col, cass_df_copy.pop(col))\n",
    "    insert_at += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a384d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cass_df = cass_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961015da",
   "metadata": {},
   "source": [
    "### Perform t-test between Gtex psi in Brain Regions and Other Regions. Save results into new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "500fd4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 13 brain region columns\n",
      "There are a total of 41 other region columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   1%|          | 404/42761 [00:00<00:20, 2019.10it/s]/Users/tianji/mambaforge/envs/myenv/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "Processing samples: 100%|| 42761/42761 [00:21<00:00, 1981.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After BH-FDR correction:\n",
      "Mann-Whitney: 17458 significant at FDR < 0.05\n",
      "Welch's t-test: 16626 significant at FDR < 0.05\n",
      "Samples (have more than 10% of Gtex psi missing) filtered out: 16998\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # significance threshold\n",
    "\n",
    "# Get Gtex psi from all regions\n",
    "Gtex_df = cass_df.loc[:,'Adipose-Subcutaneous':'Vagina'].copy()\n",
    "\n",
    "# Identify groups\n",
    "brain_region_cols = Gtex_df.loc[:, 'Brain-Amygdala':'Brain-Substantia_nigra'].columns\n",
    "print(f\"There are a total of {len(brain_region_cols)} brain region columns\")\n",
    "#print(f\"Brain region columns are: {brain_region_cols}\")\n",
    "other_region_cols = Gtex_df.columns.difference(brain_region_cols)\n",
    "print(f\"There are a total of {len(other_region_cols)} other region columns\")\n",
    "#print(f\"Other region columns are: {other_region_cols}\")\n",
    "total_cols = len(brain_region_cols) + len(other_region_cols)\n",
    "\n",
    "# Lists to store t-test results\n",
    "mw_pvals = []\n",
    "mw_significance = []\n",
    "ttest_pvals = []\n",
    "ttest_significance = []\n",
    "num_samples_filtered_out = 0\n",
    "\n",
    "##########################################################\n",
    "####################Calculate p-values####################\n",
    "##########################################################\n",
    "\n",
    "for idx, row in tqdm(Gtex_df.iterrows(), total=len(Gtex_df), desc=\"Processing samples\"):\n",
    "    # Skip if a sample misses more than 10% of the Gtex Data\n",
    "    total_columns_per_samples = row.count()\n",
    "    if total_columns_per_samples < 0.9 * total_cols:\n",
    "        mw_pvals.append(np.nan)\n",
    "        mw_significance.append(0)\n",
    "        ttest_pvals.append(np.nan)\n",
    "        ttest_significance.append(0)\n",
    "        num_samples_filtered_out += 1\n",
    "        continue\n",
    "    \n",
    "    # Extract values from each group\n",
    "    brain_region_data = row[brain_region_cols].dropna()\n",
    "    other_region_data = row[other_region_cols].dropna()  # Fixed typo in variable name\n",
    "    \n",
    "    # MannWhitney U test\n",
    "    if len(brain_region_data) >= 2 and len(other_region_data) >= 2:\n",
    "        stat_mw, pval_mw = mannwhitneyu(brain_region_data, other_region_data, alternative='two-sided')\n",
    "    else:\n",
    "        pval_mw = np.nan\n",
    "    mw_pvals.append(pval_mw)\n",
    "    mw_significance.append(1 if (not np.isnan(pval_mw) and pval_mw < alpha) else 0)\n",
    "\n",
    "    # Standard t-test (Welch's t-test assuming unequal variance)\n",
    "    if len(brain_region_data) >= 2 and len(other_region_data) >= 2:\n",
    "        stat_t, pval_t = ttest_ind(brain_region_data, other_region_data, equal_var=False)\n",
    "    else:\n",
    "        pval_t = np.nan\n",
    "    ttest_pvals.append(pval_t)\n",
    "    ttest_significance.append(1 if (not np.isnan(pval_t) and pval_t < alpha) else 0)\n",
    "\n",
    "#####################################################################################################\n",
    "####################Calculate adjust p-values with Benjamini-Hochberg (BH) method####################\n",
    "#####################################################################################################\n",
    "\n",
    "def bh_fdr_adjustment(pvals_with_nan):\n",
    "    \"\"\"Apply BH FDR correction while preserving NaN positions\"\"\"\n",
    "    # Get valid p-values and their original indices\n",
    "    valid_mask = ~np.isnan(pvals_with_nan)\n",
    "    valid_pvals = np.array(pvals_with_nan)[valid_mask]\n",
    "    \n",
    "    # Perform BH correction\n",
    "    _, fdr_pvals = multipletests(valid_pvals, alpha=alpha, method='fdr_bh')[:2]\n",
    "    \n",
    "    # Reconstruct full array with NaNs\n",
    "    adjusted = np.full_like(pvals_with_nan, np.nan, dtype=float)\n",
    "    adjusted[valid_mask] = fdr_pvals\n",
    "    return adjusted.tolist()\n",
    "\n",
    "# Calculate FDR-adjusted p-values for both tests\n",
    "mw_fdr = bh_fdr_adjustment(mw_pvals)\n",
    "ttest_fdr = bh_fdr_adjustment(ttest_pvals)\n",
    "\n",
    "# Update significance flags using FDR-adjusted values\n",
    "mw_significance_fdr = [1 if (not np.isnan(p) and p < alpha) else 0 for p in mw_fdr]\n",
    "ttest_significance_fdr = [1 if (not np.isnan(p) and p < alpha) else 0 for p in ttest_fdr]\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nAfter BH-FDR correction:\")\n",
    "print(f\"Mann-Whitney: {sum(mw_significance_fdr)} significant at FDR < {alpha}\")\n",
    "print(f\"Welch's t-test: {sum(ttest_significance_fdr)} significant at FDR < {alpha}\")\n",
    "print(f\"Samples (have more than 10% of Gtex psi missing) filtered out: {num_samples_filtered_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e5f2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert t-test results into four columns\n",
    "# 1. Find the column index of 'Adipose-Subcutaneous'\n",
    "loc_adipose_subcut = cass_df.columns.get_loc('Adipose-Subcutaneous')\n",
    "\n",
    "# 2. Insert columns at that index (the new columns will appear right BEFORE 'Adipose-Subcutaneous')\n",
    "cass_df.insert(loc_adipose_subcut, 'pvalue(mannwhitney)', mw_pvals)\n",
    "cass_df.insert(loc_adipose_subcut + 1, 'FDR(mannwhitney)', mw_fdr)\n",
    "cass_df.insert(loc_adipose_subcut + 2, 'significance(mannwhitney)', mw_significance_fdr)\n",
    "cass_df.insert(loc_adipose_subcut + 3, 'pvalue(ttest)', ttest_pvals)\n",
    "cass_df.insert(loc_adipose_subcut + 4, 'FDR(ttest)', ttest_fdr)\n",
    "cass_df.insert(loc_adipose_subcut + 5, 'significance(ttest)', ttest_significance_fdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1f134",
   "metadata": {},
   "source": [
    "### Insert in_frame column to indicate if the middle exon is dividable by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "002152a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    exon_sizes in_frame\n",
      "0   297,87,273       in\n",
      "1  128,116,144      out\n",
      "2   102,72,128       in\n",
      "3   130,65,102      out\n",
      "4   88,215,129      out\n"
     ]
    }
   ],
   "source": [
    "# Assuming cass_df is your DataFrame\n",
    "\n",
    "# 1. Extract the second substring from exon_sizes\n",
    "cass_df['exon_len2'] = cass_df['exon_sizes'].str.split(',').str[1]\n",
    "\n",
    "# 2. Convert to integer, handling errors\n",
    "cass_df['exon_len2'] = pd.to_numeric(cass_df['exon_len2'], errors='coerce')\n",
    "\n",
    "# 3. Create in_frame column based on divisibility by 3\n",
    "cass_df['in_frame'] = np.where(\n",
    "    cass_df['exon_len2'] % 3 == 0, \n",
    "    'in', \n",
    "    np.where(cass_df['exon_len2'].isna(), np.nan, 'out')\n",
    ")\n",
    "\n",
    "# 4. Find position of exon_sizes column\n",
    "exon_sizes_idx = cass_df.columns.get_loc('exon_sizes')\n",
    "\n",
    "# 5. Insert in_frame column after exon_sizes\n",
    "cols = cass_df.columns.tolist()\n",
    "cols.insert(exon_sizes_idx + 1, cols.pop(cols.index('in_frame')))\n",
    "cass_df = cass_df[cols]\n",
    "\n",
    "# Optional: Remove temporary exon_len2 column\n",
    "cass_df = cass_df.drop(columns=['exon_len2'])\n",
    "\n",
    "# Verify the result\n",
    "print(cass_df[['exon_sizes', 'in_frame']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87cd30",
   "metadata": {},
   "source": [
    "### Save all the merged data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc509b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each DataFrame as a separate CSV file\n",
    "cass_df.to_csv(\"/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Cass.Merged.DiffAlterSplicing.results.step1.csv\", index=False)\n",
    "alt3_merged_df.to_csv(\"/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Alt3.Merged.DiffAlterSplicing.results.csv\", index=False)\n",
    "alt5_merged_df.to_csv(\"/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Alt5.Merged.DiffAlterSplicing.results.csv\", index=False)\n",
    "iret_merged_df.to_csv(\"/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Iret.Merged.DiffAlterSplicing.results.csv\", index=False)\n",
    "mutx_merged_df.to_csv(\"/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Mutx.Merged.DiffAlterSplicing.results.csv\", index=False)\n",
    "taca_merged_df.to_csv(\"/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Taca.Merged.DiffAlterSplicing.results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a073e",
   "metadata": {},
   "source": [
    "# Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "244291ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/l_62hzx91kz0hng_60kps5t00000gn/T/ipykernel_62732/1496339259.py:2: DtypeWarning: Columns (230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cass_new_aluoverlap = pd.read_csv('/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Cass.AluOverlap.071825.csv')\n",
      "/var/folders/gp/l_62hzx91kz0hng_60kps5t00000gn/T/ipykernel_62732/1496339259.py:3: DtypeWarning: Columns (232) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cass_controls = pd.read_csv('/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Cass.AluOverlap.Ctrl.081325.csv')\n"
     ]
    }
   ],
   "source": [
    "cass_as_result = pd.read_csv('/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Cass.Merged.DiffAlterSplicing.results.step1.csv')\n",
    "cass_new_aluoverlap = pd.read_csv('/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Cass.AluOverlap.071825.csv')\n",
    "cass_controls = pd.read_csv('/Users/tianji/Desktop/Alu Project/Output/Merged_DSE_Results/Cass.AluOverlap.Ctrl.081325.csv')\n",
    "cols_to_remove = [\n",
    "    'alu_type', 'alu_subtype', 'alu_specific_subtype',\n",
    "    'ku_tag_summary', 'ku_bound'\n",
    "]\n",
    "\n",
    "# Drop the named ones + the last 34 columns\n",
    "cass_as_result = cass_as_result.drop(columns=cols_to_remove + list(cass_as_result.columns[-34:]))\n",
    "\n",
    "# Select columns from cass_new_aluoverlap[213:] and merge\n",
    "cols_to_merge = cass_new_aluoverlap.columns[213:]\n",
    "merged = cass_as_result.merge(\n",
    "    cass_new_aluoverlap[['name'] + list(cols_to_merge)],\n",
    "    on='name',\n",
    "    how='inner'  # ensures only common names are kept\n",
    ")\n",
    "\n",
    "# Select the last 71 columns from cass_controls\n",
    "cols_to_add = cass_controls.columns[-71:]\n",
    "merged = merged.merge(\n",
    "    cass_controls[['name'] + list(cols_to_add)],\n",
    "    on='name',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "469e89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('/Users/tianji/Desktop/Alu Project/100vs0&10vs0 Output/Cass.Merged.Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c18cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To adjust the exon_sizes column\n",
    "#merged.to_csv('/Users/tianji/Desktop/Alu Project/100vs0&10vs0 Output/Cass.Merged.Results.txt', index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
